{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling M100 Lateness\n",
    "\n",
    "Here we're trying to model the M100's lateness and simulated crowdedness in the St. Nicholas stop going to Inwood 220 St Via Amsterdam Via Bway. \n",
    "\n",
    "We are applying Datacamp's Decision-Tree for Classification\n",
    "\n",
    "## Table of Contents:\n",
    "1. [Choosing the Appropriate Classifier](#choosing-the-appropriate-classifier)\n",
    "1. [Data Cleaning](#data-cleaning)\n",
    "1. [Designing the model](#designing-the-model)\n",
    "1. [Our Model](#our-model)\n",
    "1. [Loading the Model](#loading-the-model)\n",
    "\n",
    "\\* Not finished yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prequisites (if you want to follow along/verify results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --user -U scikit-learn==0.18\n",
    "# !pip3 install --user -U seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import datetime as dt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random_state = 20181112\n",
    "import datetime, math, glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding data from the M100 csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the Appropriate Classifier\n",
    "\n",
    "We want (a) regressor(s) that can predict the **wait time** and **crowding** of a bus at a specific stop with the inputs **hourly weather** and **time of day**. We would most likely have two models that predict each **wait time** and **crowding**.\n",
    "\n",
    "Here are our top picks for regressors:\n",
    "\n",
    "1. Gradient Boosting Machines ***(top pick)***:\n",
    "    - Why: GBMs are typically a composite model that combines the efforts of multiple weak models to create a strong model, and each additional weak model reduces the mean squared error (MSE) of the overall model. Our goal would be to minimize MSE to increase the accuracy of our predictions.\n",
    "\n",
    "1. Random Forest:\n",
    "    - Why: does not suffer from the overfitting like with Decision Trees. Instead of randomly choosing to split from just **hourly weather** and **time of day**, we can have two trees that randomly split from each and find the best model. \n",
    "\n",
    "1. Decision Trees:  \n",
    "    - Reduction in Standard Deviation (metric): This is a regression metric that measures how much weâ€™ve reduced our uncertainty by picking a split point. By picking the best split each time the greedy decision tree training algorithm tries to form decisions with as few splits as possible.  \n",
    "    - Hyperparameters:   \n",
    "        * Max depth: Limit our tree to a `n` depth to prevent overfitting.\n",
    "        \n",
    "\n",
    "Evaluating our model:\n",
    "\n",
    "Since we're creating regression models, we are interested in the ***mean squared error*** and ***R Squared***. The lower our ***R Squared*** the more accurate our model. We intend to use **K-fold cross validation** as well as a **holdout set** as we improve our model through hyperparameter tuning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "> Please checkout [this notebook](../Bus_Timeline/Excel_Bus_Timeline_Draft.ipynb) on how we did the cleaning process\n",
    "\n",
    "1. Clean and break up the time components (Hour, Mins, Secs) of the following:\n",
    "    * `RecordedAtTime`\n",
    "2. Merge and store (we'll merge them based on the hour of the day and the day of the month):\n",
    "    * Bus\n",
    "        * `Hour`\n",
    "        * `Min`\n",
    "        * `Sec`\n",
    "        * `Day`\n",
    "    * Weather\n",
    "        * `Hour`\n",
    "        * `HourlyVisibility`\n",
    "        * `HourlyPrecipitation`\n",
    "        * `HourlyWindSpeed`\n",
    "3. Features of interest:\n",
    "    * `Hour`\n",
    "    * `Min`\n",
    "    * `Sec`\n",
    "    * `HourlyVisibility`\n",
    "    * `HourlyPrecipitation`\n",
    "    * `HourlyWindSpeed`\n",
    "4. Prediction result:\n",
    "    * `timeTillNext`: estimated minutes remaining until next bus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Merged_Bus_Weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training I\n",
    "\n",
    "Adapted from: https://shankarmsy.github.io/stories/gbrt-sklearn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings, seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "np.random.seed(sum(map(ord, \"aesthetics\"))) \n",
    "seaborn.set_context('notebook') \n",
    "# pd.set_option('display.mpl_style', 'default') # Make the graphs a bit prettier \n",
    "plt.rcParams['figure.figsize'] = (15, 5) # Set some Pandas options \n",
    "pd.set_option('display.notebook_repr_html', False) \n",
    "pd.set_option('display.max_columns', 40) \n",
    "pd.set_option('display.max_rows', 25) \n",
    "pd.options.display.max_colwidth = 50 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features, Targets and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "passengerArrivalTime     object\n",
       "numPassengersPerBus       int64\n",
       "BusDepartureTime         object\n",
       "HOURLYVISIBILITY        float64\n",
       "HOURLYWindSpeed         float64\n",
       "HOURLYPrecip            float64\n",
       "ArrivalHour               int64\n",
       "ArrivalSeconds            int64\n",
       "ArrivalMinutes            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11547 26.82999913397419\n",
      "2887 27.04052649809491\n"
     ]
    }
   ],
   "source": [
    "features = (['ArrivalHour', 'ArrivalSeconds', 'ArrivalMinutes', \n",
    "             'HOURLYVISIBILITY', 'HOURLYWindSpeed', 'HOURLYPrecip'])\n",
    "\n",
    "target = 'numPassengersPerBus'\n",
    "\n",
    "model_df = df[(features + [target])].dropna().reset_index()\n",
    "\n",
    "train_df, holdout_df, y_train, y_holdout = train_test_split(\n",
    "    model_df[features], \n",
    "    model_df[target], test_size=0.2,\n",
    "    random_state=random_state)\n",
    "\n",
    "train_df[target] = y_train\n",
    "holdout_df[target] = y_holdout\n",
    "\n",
    "train_df.reset_index(inplace=True)\n",
    "holdout_df.reset_index(inplace=True)\n",
    "\n",
    "print(train_df.shape[0], train_df.numPassengersPerBus.mean())\n",
    "print(holdout_df.shape[0], holdout_df.numPassengersPerBus.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df\n",
    "X_test = holdout_df\n",
    "y_train = y_train\n",
    "y_test = y_holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gradient Boosting Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt=GradientBoostingRegressor(n_estimators=100) \n",
    "train_df.shape\n",
    "\n",
    "gbrt.fit(train_df, y_train) \n",
    "y_pred=gbrt.predict(holdout_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Metrics\n",
    "\n",
    "We're measure the following:\n",
    "* R Squared : explains how well the independent variables explain the variability in the dependent.\n",
    "* **Mean Squared Error:** a risk metric corresponding to the expected value of the squared (quadratic) error or loss. It is an estimator that measures the average of the squares of the errors. \n",
    "    \n",
    "We're concerned about minimizing MSE as it leads to predictions that are close to the population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=10, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our hyperparameter should be here.  \n",
    "NOTE: DO NOT RUN UNLESS YOU ARE WILLING TO SPEND > 2 HRS ON CALCULATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 4 Splits: 20 Rate: 0.1 n_estimators: 200 199.08979045383472 27.743574326432228\n",
      "Depth: 4 Splits: 20 Rate: 0.1 n_estimators: 400 195.87222997968576 28.695973599225702\n",
      "Depth: 4 Splits: 20 Rate: 0.1 n_estimators: 600 195.0206280215395 27.68516460653923\n",
      "Depth: 4 Splits: 20 Rate: 0.1 n_estimators: 800 196.13702306024717 27.713527852404297\n",
      "Depth: 4 Splits: 20 Rate: 0.05 n_estimators: 200 205.768138906593 27.68032162934012\n",
      "Depth: 4 Splits: 20 Rate: 0.05 n_estimators: 400 199.3785383582092 27.327002414283733\n",
      "Depth: 4 Splits: 20 Rate: 0.05 n_estimators: 600 196.88164015596283 27.726851981196273\n",
      "Depth: 4 Splits: 20 Rate: 0.05 n_estimators: 800 196.08449372788883 27.791080941501143\n",
      "Depth: 4 Splits: 20 Rate: 0.01 n_estimators: 200 276.6003638466901 34.509902854386986\n",
      "Depth: 4 Splits: 20 Rate: 0.01 n_estimators: 400 227.54505950769402 30.23556738561684\n",
      "Depth: 4 Splits: 20 Rate: 0.01 n_estimators: 600 214.6636742401838 28.13481326286243\n",
      "Depth: 4 Splits: 20 Rate: 0.01 n_estimators: 800 209.59384999816461 28.051041297569792\n",
      "Depth: 4 Splits: 40 Rate: 0.1 n_estimators: 200 198.76423867671355 27.331383958764494\n",
      "Depth: 4 Splits: 40 Rate: 0.1 n_estimators: 400 194.89950947124422 26.671084143290916\n",
      "Depth: 4 Splits: 40 Rate: 0.1 n_estimators: 600 193.85111101813817 26.536890466263195\n",
      "Depth: 4 Splits: 40 Rate: 0.1 n_estimators: 800 194.41753150522584 26.540301424915288\n",
      "Depth: 4 Splits: 40 Rate: 0.05 n_estimators: 200 206.32780121194628 28.266436623046722\n",
      "Depth: 4 Splits: 40 Rate: 0.05 n_estimators: 400 199.779454943737 27.099555253236264\n",
      "Depth: 4 Splits: 40 Rate: 0.05 n_estimators: 600 197.28458666264333 26.938503552286758\n",
      "Depth: 4 Splits: 40 Rate: 0.05 n_estimators: 800 196.6849355821826 26.99402812028518\n",
      "Depth: 4 Splits: 40 Rate: 0.01 n_estimators: 200 276.5871325391827 34.5151769543909\n",
      "Depth: 4 Splits: 40 Rate: 0.01 n_estimators: 400 227.6610128716984 29.93782312533859\n",
      "Depth: 4 Splits: 40 Rate: 0.01 n_estimators: 600 215.10324580276023 28.418762341211256\n",
      "Depth: 4 Splits: 40 Rate: 0.01 n_estimators: 800 210.2488405891576 27.778418367381548\n",
      "Depth: 4 Splits: 60 Rate: 0.1 n_estimators: 200 200.55402165723962 28.529445137602945\n",
      "Depth: 4 Splits: 60 Rate: 0.1 n_estimators: 400 196.514296888566 28.12829879680189\n",
      "Depth: 4 Splits: 60 Rate: 0.1 n_estimators: 600 195.6437790328186 27.225948649163467\n",
      "Depth: 4 Splits: 60 Rate: 0.1 n_estimators: 800 196.0938511858106 27.78508342673514\n",
      "Depth: 4 Splits: 60 Rate: 0.05 n_estimators: 200 205.86254253728504 29.33437406507526\n",
      "Depth: 4 Splits: 60 Rate: 0.05 n_estimators: 400 199.68571770551884 27.359975660250385\n",
      "Depth: 4 Splits: 60 Rate: 0.05 n_estimators: 600 196.61288117303803 27.207609471727825\n",
      "Depth: 4 Splits: 60 Rate: 0.05 n_estimators: 800 195.5093332276186 26.65772755301133\n",
      "Depth: 4 Splits: 60 Rate: 0.01 n_estimators: 200 276.6505394588793 34.52268184430857\n",
      "Depth: 4 Splits: 60 Rate: 0.01 n_estimators: 400 228.0921292737159 30.36951147109032\n",
      "Depth: 4 Splits: 60 Rate: 0.01 n_estimators: 600 215.84571102278792 29.10549447368698\n",
      "Depth: 4 Splits: 60 Rate: 0.01 n_estimators: 800 210.9601994384653 28.993535761729156\n",
      "Depth: 4 Splits: 80 Rate: 0.1 n_estimators: 200 199.7953210170433 27.495142492484327\n",
      "Depth: 4 Splits: 80 Rate: 0.1 n_estimators: 400 195.7080137006118 26.89066553009732\n",
      "Depth: 4 Splits: 80 Rate: 0.1 n_estimators: 600 195.7475782560985 26.68963363546814\n",
      "Depth: 4 Splits: 80 Rate: 0.1 n_estimators: 800 195.78931491693368 26.54688753588107\n",
      "Depth: 4 Splits: 80 Rate: 0.05 n_estimators: 200 206.53274022891975 27.679740901744083\n",
      "Depth: 4 Splits: 80 Rate: 0.05 n_estimators: 400 199.81389613861978 26.869553554487748\n",
      "Depth: 4 Splits: 80 Rate: 0.05 n_estimators: 600 197.27862727376993 27.49365929695401\n",
      "Depth: 4 Splits: 80 Rate: 0.05 n_estimators: 800 195.93181829102903 26.987038827172682\n",
      "Depth: 4 Splits: 80 Rate: 0.01 n_estimators: 200 277.0924620811552 34.49753075120053\n",
      "Depth: 4 Splits: 80 Rate: 0.01 n_estimators: 400 230.36472832682043 30.5740285570682\n",
      "Depth: 4 Splits: 80 Rate: 0.01 n_estimators: 600 216.60873368682002 28.487517675606714\n",
      "Depth: 4 Splits: 80 Rate: 0.01 n_estimators: 800 211.79029500533198 27.994619578560158\n",
      "Depth: 6 Splits: 20 Rate: 0.1 n_estimators: 200 185.32931185907557 27.946790593337177\n",
      "Depth: 6 Splits: 20 Rate: 0.1 n_estimators: 400 192.75016695864076 28.382993245684528\n",
      "Depth: 6 Splits: 20 Rate: 0.1 n_estimators: 600 201.0915295022047 29.907563082983177\n",
      "Depth: 6 Splits: 20 Rate: 0.1 n_estimators: 800 207.5502090892186 31.031582036422297\n",
      "Depth: 6 Splits: 20 Rate: 0.05 n_estimators: 200 182.13397900044907 27.450825424279508\n",
      "Depth: 6 Splits: 20 Rate: 0.05 n_estimators: 400 186.13564666686963 28.585008646794417\n",
      "Depth: 6 Splits: 20 Rate: 0.05 n_estimators: 600 189.87789557459223 28.519220259636207\n",
      "Depth: 6 Splits: 20 Rate: 0.05 n_estimators: 800 193.98855575373426 29.02423512485189\n",
      "Depth: 6 Splits: 20 Rate: 0.01 n_estimators: 200 215.51502093578296 31.64233892790598\n",
      "Depth: 6 Splits: 20 Rate: 0.01 n_estimators: 400 183.9241383707635 27.498474084238406\n",
      "Depth: 6 Splits: 20 Rate: 0.01 n_estimators: 600 180.5942206926847 28.568755985885446\n",
      "Depth: 6 Splits: 20 Rate: 0.01 n_estimators: 800 180.65422311970354 28.857054709710237\n",
      "Depth: 6 Splits: 40 Rate: 0.1 n_estimators: 200 186.60138259659263 29.526337505181594\n",
      "Depth: 6 Splits: 40 Rate: 0.1 n_estimators: 400 192.345476951434 30.372556511055727\n",
      "Depth: 6 Splits: 40 Rate: 0.1 n_estimators: 600 198.5875209176914 31.19144631830383\n",
      "Depth: 6 Splits: 40 Rate: 0.1 n_estimators: 800 204.30457877257282 31.794075356007138\n",
      "Depth: 6 Splits: 40 Rate: 0.05 n_estimators: 200 182.15763988999993 28.753220351511416\n",
      "Depth: 6 Splits: 40 Rate: 0.05 n_estimators: 400 184.56453453142052 29.165784035518303\n",
      "Depth: 6 Splits: 40 Rate: 0.05 n_estimators: 600 185.965538035643 29.57005663848227\n",
      "Depth: 6 Splits: 40 Rate: 0.05 n_estimators: 800 188.83267014335948 29.49691423075882\n",
      "Depth: 6 Splits: 40 Rate: 0.01 n_estimators: 200 220.1069870909292 32.1355913655268\n",
      "Depth: 6 Splits: 40 Rate: 0.01 n_estimators: 400 186.83942484582602 28.046496519512754\n",
      "Depth: 6 Splits: 40 Rate: 0.01 n_estimators: 600 181.7855696564976 28.69825771591488\n",
      "Depth: 6 Splits: 40 Rate: 0.01 n_estimators: 800 181.38601280351403 28.216855094177127\n",
      "Depth: 6 Splits: 60 Rate: 0.1 n_estimators: 200 184.23109102476585 26.852594924763835\n",
      "Depth: 6 Splits: 60 Rate: 0.1 n_estimators: 400 188.47376065220644 27.26958346418093\n",
      "Depth: 6 Splits: 60 Rate: 0.1 n_estimators: 600 193.30260289502945 27.845627326448582\n",
      "Depth: 6 Splits: 60 Rate: 0.1 n_estimators: 800 197.4354973975938 28.07686450967442\n",
      "Depth: 6 Splits: 60 Rate: 0.05 n_estimators: 200 181.24550011354566 27.17180750109216\n",
      "Depth: 6 Splits: 60 Rate: 0.05 n_estimators: 400 182.42710176425763 27.45705738861762\n",
      "Depth: 6 Splits: 60 Rate: 0.05 n_estimators: 600 185.27088135283094 28.026189911856843\n",
      "Depth: 6 Splits: 60 Rate: 0.05 n_estimators: 800 187.039125597537 28.778879657332123\n",
      "Depth: 6 Splits: 60 Rate: 0.01 n_estimators: 200 222.61776589450506 30.248047781079872\n",
      "Depth: 6 Splits: 60 Rate: 0.01 n_estimators: 400 188.24326556749887 26.91046436609381\n",
      "Depth: 6 Splits: 60 Rate: 0.01 n_estimators: 600 182.59816789033675 26.703641082439788\n",
      "Depth: 6 Splits: 60 Rate: 0.01 n_estimators: 800 181.48914649350348 26.304065171157504\n",
      "Depth: 6 Splits: 80 Rate: 0.1 n_estimators: 200 183.67059079135225 27.458969693629722\n",
      "Depth: 6 Splits: 80 Rate: 0.1 n_estimators: 400 187.31787706958113 27.498158466762387\n",
      "Depth: 6 Splits: 80 Rate: 0.1 n_estimators: 600 191.47885041974232 28.282521058934503\n",
      "Depth: 6 Splits: 80 Rate: 0.1 n_estimators: 800 197.15615068087376 28.87007003156234\n",
      "Depth: 6 Splits: 80 Rate: 0.05 n_estimators: 200 183.15566463797947 28.354992678533648\n",
      "Depth: 6 Splits: 80 Rate: 0.05 n_estimators: 400 183.47986588726968 28.565322367344525\n",
      "Depth: 6 Splits: 80 Rate: 0.05 n_estimators: 600 185.52974834187415 27.909097461847235\n",
      "Depth: 6 Splits: 80 Rate: 0.05 n_estimators: 800 187.29316786328968 27.754765798841355\n",
      "Depth: 6 Splits: 80 Rate: 0.01 n_estimators: 200 225.73927572998846 31.89223262899545\n",
      "Depth: 6 Splits: 80 Rate: 0.01 n_estimators: 400 190.21514846187148 27.947149568045294\n",
      "Depth: 6 Splits: 80 Rate: 0.01 n_estimators: 600 184.96600862124774 28.618833780299987\n",
      "Depth: 6 Splits: 80 Rate: 0.01 n_estimators: 800 183.59645444800316 28.18546821287616\n",
      "Depth: 8 Splits: 20 Rate: 0.1 n_estimators: 200 192.88254573131584 30.344995094998236\n",
      "Depth: 8 Splits: 20 Rate: 0.1 n_estimators: 400 207.11868225900577 32.4735331909133\n",
      "Depth: 8 Splits: 20 Rate: 0.1 n_estimators: 600 216.0810420015849 33.17665835170277\n",
      "Depth: 8 Splits: 20 Rate: 0.1 n_estimators: 800 222.41493056488903 33.72231274727157\n",
      "Depth: 8 Splits: 20 Rate: 0.05 n_estimators: 200 181.29930203512566 29.76420757367459\n",
      "Depth: 8 Splits: 20 Rate: 0.05 n_estimators: 400 190.5504580898425 30.922385932637106\n",
      "Depth: 8 Splits: 20 Rate: 0.05 n_estimators: 600 197.61771482290388 31.67513264396678\n",
      "Depth: 8 Splits: 20 Rate: 0.05 n_estimators: 800 203.91278243774138 32.74029133345812\n",
      "Depth: 8 Splits: 20 Rate: 0.01 n_estimators: 200 199.89128642645719 30.3642025005229\n",
      "Depth: 8 Splits: 20 Rate: 0.01 n_estimators: 400 175.6289887200698 29.394141889506297\n",
      "Depth: 8 Splits: 20 Rate: 0.01 n_estimators: 600 176.67140158449916 29.528135427955956\n",
      "Depth: 8 Splits: 20 Rate: 0.01 n_estimators: 800 179.07046398107596 29.987382502692228\n",
      "Depth: 8 Splits: 40 Rate: 0.1 n_estimators: 200 188.427630779274 31.163115875022548\n",
      "Depth: 8 Splits: 40 Rate: 0.1 n_estimators: 400 201.0138069346859 34.19253848319278\n",
      "Depth: 8 Splits: 40 Rate: 0.1 n_estimators: 600 210.54714799756675 35.70804027375207\n",
      "Depth: 8 Splits: 40 Rate: 0.1 n_estimators: 800 217.75222947280236 36.40525958563033\n",
      "Depth: 8 Splits: 40 Rate: 0.05 n_estimators: 200 178.51657936796298 29.239010066694657\n",
      "Depth: 8 Splits: 40 Rate: 0.05 n_estimators: 400 186.6532247465174 30.119307322609508\n",
      "Depth: 8 Splits: 40 Rate: 0.05 n_estimators: 600 194.0640281004791 31.20801955782381\n",
      "Depth: 8 Splits: 40 Rate: 0.05 n_estimators: 800 199.6250890630736 32.07872517775418\n",
      "Depth: 8 Splits: 40 Rate: 0.01 n_estimators: 200 200.94050516438276 28.858983115966602\n",
      "Depth: 8 Splits: 40 Rate: 0.01 n_estimators: 400 176.44051358933672 27.294328506590272\n",
      "Depth: 8 Splits: 40 Rate: 0.01 n_estimators: 600 175.60076094812706 27.082691298358856\n",
      "Depth: 8 Splits: 40 Rate: 0.01 n_estimators: 800 176.5127130831109 27.23429637936629\n",
      "Depth: 8 Splits: 60 Rate: 0.1 n_estimators: 200 183.75574622668236 28.376763591512606\n",
      "Depth: 8 Splits: 60 Rate: 0.1 n_estimators: 400 196.58024418330737 30.118499446953823\n",
      "Depth: 8 Splits: 60 Rate: 0.1 n_estimators: 600 205.1160452316306 31.45104155328683\n",
      "Depth: 8 Splits: 60 Rate: 0.1 n_estimators: 800 212.6862144496907 32.68426277322605\n",
      "Depth: 8 Splits: 60 Rate: 0.05 n_estimators: 200 178.18932825773692 28.345162596061034\n",
      "Depth: 8 Splits: 60 Rate: 0.05 n_estimators: 400 184.32629106779999 28.38243186902106\n",
      "Depth: 8 Splits: 60 Rate: 0.05 n_estimators: 600 189.31651899543664 29.94753949614487\n",
      "Depth: 8 Splits: 60 Rate: 0.05 n_estimators: 800 194.67566978136006 30.385643826640745\n",
      "Depth: 8 Splits: 60 Rate: 0.01 n_estimators: 200 203.52430473985663 28.65712771871299\n",
      "Depth: 8 Splits: 60 Rate: 0.01 n_estimators: 400 177.38963878708682 26.365491781496576\n",
      "Depth: 8 Splits: 60 Rate: 0.01 n_estimators: 600 176.71367540453963 26.383792754878257\n",
      "Depth: 8 Splits: 60 Rate: 0.01 n_estimators: 800 177.10880122626753 26.86887002838293\n",
      "Depth: 8 Splits: 80 Rate: 0.1 n_estimators: 200 183.99586881250144 27.863756025546056\n",
      "Depth: 8 Splits: 80 Rate: 0.1 n_estimators: 400 194.7081807777701 28.580813920124914\n",
      "Depth: 8 Splits: 80 Rate: 0.1 n_estimators: 600 203.86344704921729 29.852340614993473\n",
      "Depth: 8 Splits: 80 Rate: 0.1 n_estimators: 800 211.61572056271933 30.707781880501905\n",
      "Depth: 8 Splits: 80 Rate: 0.05 n_estimators: 200 176.66412857101187 27.68822076832791\n",
      "Depth: 8 Splits: 80 Rate: 0.05 n_estimators: 400 181.7499899194023 27.49649718621822\n",
      "Depth: 8 Splits: 80 Rate: 0.05 n_estimators: 600 187.07262292351396 27.771529858227243\n",
      "Depth: 8 Splits: 80 Rate: 0.05 n_estimators: 800 191.44814321065235 27.850467360835314\n",
      "Depth: 8 Splits: 80 Rate: 0.01 n_estimators: 200 207.31728200514036 32.43025049608819\n",
      "Depth: 8 Splits: 80 Rate: 0.01 n_estimators: 400 178.80404453008276 28.8665925487281\n",
      "Depth: 8 Splits: 80 Rate: 0.01 n_estimators: 600 176.37835107682147 28.423231032868845\n",
      "Depth: 8 Splits: 80 Rate: 0.01 n_estimators: 800 176.40273607288867 28.333380707027164\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.1, 0.05, 0.01]\n",
    "min_samples_splits = range(20, 100, 20)\n",
    "max_depths = [4, 6, 8]\n",
    "n_estimators = range(200,1000,200)\n",
    "\n",
    "all_mu = []\n",
    "all_sigma = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    for min_splits in min_samples_splits:\n",
    "        for rate in learning_rates:\n",
    "            for est in n_estimators:\n",
    "                print(\"Depth:\", depth, \"Splits:\", min_splits, \"Rate:\", rate, \"n_estimators:\", est, end=\" \")\n",
    "                gbrm=GradientBoostingRegressor(\n",
    "                    random_state=random_state, \n",
    "                    max_depth=depth,\n",
    "                    min_samples_split=min_splits,\n",
    "                    learning_rate=rate,\n",
    "                    n_estimators=est\n",
    "                )\n",
    "\n",
    "                mu, sigma = get_cv_results(gbrm)\n",
    "                all_mu.append(mu)\n",
    "                all_sigma.append(sigma)\n",
    "\n",
    "                print(mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we've picked:  \n",
    "```\n",
    "max_depth=8,  \n",
    "min_samples_split=40,  \n",
    "learning_rate=0.01,  \n",
    "n_estimators=600  \n",
    "   \n",
    "```\n",
    "\n",
    "as our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_results(regressor):\n",
    "    \n",
    "    mse = []\n",
    "    r2_scores = []\n",
    "    \n",
    "    for train, test in k_fold.split(train_df):\n",
    "        regressor.fit(train_df.loc[train, features], train_df.loc[train, target])\n",
    "        y_predicted = regressor.predict(train_df.loc[test, features])\n",
    "        \n",
    "        mean_squared = mean_squared_error(train_df.loc[test, target], y_predicted)\n",
    "        mse.append(mean_squared)\n",
    "        \n",
    "        r2 = r2_score(train_df.loc[test, target], y_predicted)\n",
    "        r2_scores.append(r2)\n",
    "    \n",
    "    return (np.mean(mse), np.std(mse), \n",
    "            np.mean(r2_scores), np.std(r2_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of mean squared error: 175.60076094812706\n",
      "Mean squared error std: 27.082691298358856\n",
      "Mean of R Squared: 0.8724802563070385\n",
      "R Squared std: 0.015549400642375276\n"
     ]
    }
   ],
   "source": [
    "gbm = GradientBoostingRegressor(\n",
    "    random_state=random_state, \n",
    "    learning_rate = 0.01,\n",
    "    min_samples_split=40,\n",
    "    max_depth=8,\n",
    "    n_estimators=600\n",
    ")\n",
    "\n",
    "results = get_cv_results(gbm)\n",
    "\n",
    "print(\"Mean of mean squared error:\", results[0])\n",
    "print(\"Mean squared error std:\", results[1])\n",
    "print(\"Mean of R Squared:\", results[2])\n",
    "print(\"R Squared std:\", results[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our Model\n",
    "Pretty good results. Now let's save this model for future use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hamlet_Crowding_Model = gbm\n",
    "\n",
    "for train, test in k_fold.split(train_df):\n",
    "    Hamlet_Crowding_Model.fit(train_df.loc[train, features], train_df.loc[train, target])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U --user joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/GBRT_Hamlet_Crowding.joblib']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving\n",
    "from joblib import dump, load\n",
    "dump(Hamlet_Crowding_Model, '../data/GBRT_Hamlet_Crowding.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    @params: \n",
    "        timeNow: The current time of day the person arrived to the bus stop\n",
    "        currentHourlyWeather: The current hourly weather data. Has to have the following:\n",
    "            + Hourly Precipitation\n",
    "            + Hourly Visibility\n",
    "            + Hourly Windspeeds\n",
    "'''\n",
    "def predictCrowding(timeNow, currentHourlyWeather):\n",
    "    currData = pd.Series([])\n",
    "    currData[\"ArrivalHour\"] = timeNow.split(\":\")[0]\n",
    "    currData[\"ArrivalMinutes\"] = timeNow.split(\":\")[1]\n",
    "    currData[\"ArrivalSeconds\"] = timeNow.split(\":\")[2]\n",
    "    currData[\"HOURLYVISIBILITY\"] = currentHourlyWeather[0]\n",
    "    currData[\"HOURLYWindSpeed\"] = currentHourlyWeather[1]\n",
    "    currData[\"HOURLYPrecip\"] = currentHourlyWeather[2]\n",
    "    \n",
    "    return Hamlet_Crowding_Model.predict(currData)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sample data:\n",
    "    \n",
    "    19, 31, 39, 4.0, 6.0, 0.03 => 7\n",
    "    0, 2, 48, 10.0, 6.0, 0.03, => 7  \n",
    "    17, 14, 13, 10.0, 5.0, 0.00, => 8  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.107071301826297"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeNow = \"19:31:39\"\n",
    "weatherNow = [4.0, 6.0, 0.03]\n",
    "\n",
    "predictCrowding(timeNow, weatherNow)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.680940892445431"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeNow = \"00:02:48\"\n",
    "weatherNow = [10.0, 6.0, 0.03]\n",
    "\n",
    "predictCrowding(timeNow, weatherNow)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.4159927796028535"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeNow = \"17:14:13\"\n",
    "weatherNow = [10.0, 5.0, 0.00]\n",
    "\n",
    "predictCrowding(timeNow, weatherNow)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Model\n",
    "\n",
    "\n",
    "Credit: https://scikit-learn.org/stable/modules/model_persistence.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U --user joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading\n",
    "model = load('../../data/GBRT_Hamlet.joblib') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
