{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HamletMTA Final Report\n",
    "\n",
    "- [David Hadaller](#David-Hadaller)\n",
    "- [Angelika Shastapalava](#Angelika-Shastapalava)\n",
    "- [Sam Mundle](#Sam-Mundle)\n",
    "- [Excel Espina](#Excel-Espina)\n",
    "\n",
    "## Project Aims\n",
    "\n",
    "We will use MTA bus data, Monte Carlo passenger simulation and weather data to create a model that will determine how long a passenger should wait for the next bus, before giving up and chosing an alternative mode of transportation. Our focus thus far has been on one particular bus stop (the southbound M100 on 135th street) in August of 2017. However, this model could be iterated system-wide for every bus stop location. \n",
    "\n",
    "The diagram below serves as a reference for our analysis and is explained in more detail in the passages that follow.\n",
    "\n",
    "![diagram](analysis_diagram.png)\n",
    "\n",
    "### Monte Carlo Simulation\n",
    "\n",
    "In our Monte Carlo simulation, we will first assume that there is a uniform probability that a passenger will approach a stop at any given time of day. The reason for this choice is for it's realism (it is a common assumtion among staff analysts at the MTA's operations and planning department) and for it's ease of computation. Later on, we can experiment different ridership behaviors such as a bimodal probability distribution (peak in the early morning and evening for commuters). \n",
    "\n",
    "#### Passenger Wait Times ([Sam](#Sam-Mundle))\n",
    "\n",
    "We view each day's bus arrivals, at a given bus stop, as points on a timeline. Similarly, each  passenger arival at the bus stop falls also on the day's timeline. Calculating wait times then amounts to finding the difference between the passenger's arrival and the arrival of the very next bus along this timeline. Our goal here is to create a dataset with the time (as in time of day) of a passenger's approach to the bus station as the independent variable and the wait time as the dependent variable. This data will serve as the validation set for our model.\n",
    "\n",
    "#### Bus Time Deltas/headways ([David](#David-Hadaller))\n",
    "\n",
    "For the simulated passengers, each approaching the bus stop at some random time, we would like to find how crowded the bus they board will be. To find this, we will simulate many passengers \"boarding\" busses by assigning a population of $n$ daily passengers a uniform random timestamp between the last and first busses of the day. Buses which feature an abnormally long timedelta (long time interval between current and last bus arrival) will generally be more crowded, since more passengers accumulate at the bus stop as time goes on. Again, the time of day is our independent variable and the number of people on the bus (crowding) is the dependent variable.  This data will serve as the validation set for our model.\n",
    "\n",
    "### Weather Data ([Angelika](#Angelika-Shastapalava))\n",
    "\n",
    "Weather data will include columns for precipitation, wind speed, and visibility for the month of August 2017 in NYC. These will serve as our features for predicting crowding and wait time.\n",
    "\n",
    "\n",
    "### Machine Learning Model ([Excel](#Excel-Espina))\n",
    "\n",
    "The feature variables will be include weather data columns and time of day, while the target variables will be the crowding and wait times as experienced by the passengers (to predict what we simulated in the Monte Carlo method step.) Our goal is to produce a general weather-conscious model that predicts passenger experience (wait time and crowdedness)\n",
    "\n",
    "Considerations for this step include, but are not limited to:\n",
    "\n",
    "- model evaluation (accuracrecall,prediction, lift and all that)\n",
    "- model type (linear, non linear/ regression/decision tree)\n",
    "\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "[MTA Schedules](http://web.mta.info/nyct/service/bus/bklnsch.htm#top) (need to figure out best way to scrape or source better structured data)\n",
    "\n",
    "[MTA Bus Statistics](https://www.kaggle.com/stoney71/new-york-city-transport-statistics)\n",
    "\n",
    "[Weather.Gov](https://www.weather.gov/okx/CentralParkHistorical) Data from a weather monitor in central park; Each day, a 1:30 am report holds 24 hours of weather data starting at 12:00 am EST the previous day and the reports look like [this](https://forecast.weather.gov/product.php?site=NWS&issuedby=NYC&product=CLI&format=CI&version=1&glossary=1&highlight=off)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# David Hadaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "\n",
    "import matplotlib.dates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a dataset for the timestamps of busses at the Amsterdam & 125th st stop on M100 line going uptown \n",
    "busArrivals = pd.read_csv(\"../data/Arrivals_M100.csv\",index_col=0)\n",
    "\n",
    "# loopTime is the minimum amount of time, in minutes, that it takes a bus to complete the bus route and \n",
    "    # arrive at this stop to complete the circuit once again\n",
    "loopTime = datetime.timedelta(minutes=105)\n",
    "\n",
    "# Ensure ordering by VehicleRef (a vehicle identifier for busses) and RecordedAtTime (timestamps)\n",
    "busArrivals = busArrivals.sort_values(['VehicleRef','RecordedAtTime'])\n",
    "\n",
    "# Resetting Index and deleting resulting index column after ordering for shift later on.\n",
    "busArrivals = busArrivals.reset_index()\n",
    "busArrivals.drop(columns=['index',],inplace=True)\n",
    "\n",
    "# Ensure that RecordedAtTime is of correct data type to find timedelta\n",
    "busArrivals['RecordedAtTime'] = pd.to_datetime(busArrivals['RecordedAtTime'])\n",
    "\n",
    "# find difference between CURRENT timestamp and PREVIOUS for each gps-timestamp\n",
    "    #busArrivals['timeDelta'] = busArrivals_Grouped['RecordedAtTime'].diff()\n",
    "busArrivals['timeDelta'] = busArrivals['RecordedAtTime'].diff()\n",
    "\n",
    "# we want to find all the timestamps where busses pull away from this one stop. \n",
    "    # the departure time is when we consider that a passenger is no longer waiting for their journey to start. \n",
    "    # hence, we count bus idleing as part of the passengers experienced wait time\n",
    "\n",
    "# wherever the difference between two consecutive timestamps is greater than the loopTime, \n",
    "    # the bus has finished it's route and come back to the same stop it started at.\n",
    "    # The bus is not idleing.  \n",
    "busArrivals['hasLooped'] = busArrivals['timeDelta'] > loopTime\n",
    "\n",
    "#fixing some edge cases e.g. the first datapoint has no timedeta because no other time precedes it\n",
    "busArrivals.loc[0,'timeDelta'] = 0\n",
    "\n",
    "# # where the timedeta is NaT, set to haslooped=True. We do this so that the first Entry for a given Vehicleref won't \n",
    "#     # count as a departure time, but the last entry from the previous VehicleRef entry will.\n",
    "# busArrivals.loc[busArrivals['timeDelta'].isnull(),'hasLooped'] = True\n",
    "\n",
    "# wherever the next arrival is a Looparound, the current timestamp is considered a departure from the stop\n",
    "busArrivals['isDeparting'] = busArrivals['hasLooped'].shift(-1)\n",
    "\n",
    "#the last entry in the entire dataframe must be included as a departure\n",
    "busArrivals.loc[busArrivals.index[-1], 'isDeparting']= True\n",
    "\n",
    "# If the next bus is not the same as the current bus, then this entry must be considered a departure\n",
    "busArrivals['NextVehicleRef'] = busArrivals['VehicleRef'].shift(-1).fillna(\"\") #create next bus column by shifting current bus up by 1 relative to index\n",
    "mask = busArrivals['VehicleRef'] != busArrivals['NextVehicleRef']\n",
    "busArrivals.loc[mask, 'isDeparting'] = True\n",
    "\n",
    "# return all rows where the busses are departing\n",
    "busArrivals = busArrivals[busArrivals['isDeparting'] != False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#M100_NICK = pd.read_csv('../data/M100_month_W125_st.csv')\n",
    "M100_NICK = busArrivals\n",
    "M100_NICK.columns\n",
    "M100_NICK.shape\n",
    "M100_NICK.to_csv(\"../data/testing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin the simulation, we need to establish what will become the arguments to `numpy.random.uniform(low=0.0, high=1.0, size=None)`. Below, we find the `low` and `high` parameters. That is, we find the first and last bus arrival times for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to change the datetime to string to apply string split methods\n",
    "busArrivals['RecordedAtTime'] = busArrivals['RecordedAtTime'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "DailyBusMinMax= M100_NICK.loc[:,['RecordedAtTime']]\n",
    "splitCol = DailyBusMinMax['RecordedAtTime'].str.split(' ', 1, expand=True).rename(columns={0:'Date', 1:'Time'}) \n",
    "DailyBusMinMax['Date']= splitCol['Date'] \n",
    "\n",
    "DailyBusMinMax = DailyBusMinMax.drop_duplicates()\n",
    "\n",
    "DailyBusMax = DailyBusMinMax.groupby('Date').max()\n",
    "DailyBusMin = DailyBusMinMax.groupby('Date').min()\n",
    "\n",
    "DailyBusMinMax = pd.merge(left=DailyBusMin, right=DailyBusMax, how='inner',on='Date', suffixes=('Min', 'Max'))\n",
    "DailyBusMinMax = DailyBusMinMax.rename(columns={'RecordedAtTimeMin':'EarliestBusArrival', 'RecordedAtTimeMax':'LatestBusArrival'})\n",
    "\n",
    "DailyBusMinMax.reset_index(level=0, inplace=True)\n",
    "DailyBusMinMax['Date'] = pd.to_datetime(DailyBusMinMax['Date'],format='%Y-%m-%d')\n",
    "DailyBusMinMax['EarliestBusArrival'] = pd.to_datetime(DailyBusMinMax['EarliestBusArrival'], format='%Y-%m-%d %H:%M:%S')\n",
    "DailyBusMinMax['LatestBusArrival'] = pd.to_datetime(DailyBusMinMax['LatestBusArrival'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "DailyBusMinMax.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we take the bus arrival times (when the bus pulls into the stop), the dates associated with each bus arrival time (to help with subsequent merges) and the timedeltas (which we may plot later on) from the original `M100_NICK` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BusArrivals = M100_NICK.loc[:,['RecordedAtTime','time_delta_mins']]\n",
    "dates = M100_NICK['RecordedAtTime'].str.split(' ', 1, expand=True).rename(columns={0:'Date'})\n",
    "BusArrivals.insert(loc=0, column='Date', value=dates['Date'])\n",
    "\n",
    "BusArrivals = BusArrivals.rename(columns={'RecordedAtTime':'BusArrivalTime'})\n",
    "\n",
    "# Change Column DataTypes from String (object) to DateTime\n",
    "BusArrivals['BusArrivalTime'] = pd.to_datetime(BusArrivals['BusArrivalTime'], format='%Y-%m-%d %H:%M:%S')\n",
    "BusArrivals['Date'] = pd.to_datetime(BusArrivals['Date'], format='%Y-%m-%d %H:%M:%S')\n",
    "BusArrivals = BusArrivals.drop_duplicates()\n",
    "\n",
    "BusArrivals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a simulation function which gets the `low` and `high` bounds of the uniform distribution from the DailyBusMinMax dataframe and then takes `NumPassengers` for population size of passengers to simulate. \n",
    "\n",
    "This simulation function creates a \"pivot table\" with the Date as pseudo-index and an artificial passengerId for column headers. For each date, the table contains each passenger in NumPassengers simulated bus arrival time (the time at which each passenger approaches the bus stop with the hopes of boarding a bus.)\n",
    "\n",
    "Of course, the table that results isn't a true pivot table, because the Date column is just another column, rather than a pandas index. Keeping the Date as a column allows us to reference it as a column later on, which will come in handy when we need to return a series of dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passengerSim(DailyBusMinMax, NumPassengers):\n",
    "    \n",
    "    #time between the first and last bus arrivals\n",
    "    dailyDelta = DailyBusMinMax['LatestBusArrival'] - DailyBusMinMax['EarliestBusArrival']\n",
    "    \n",
    "    # the first bus arrival\n",
    "    dailyMin = DailyBusMinMax['EarliestBusArrival']\n",
    "    \n",
    "    #number of dates to simulate for\n",
    "    NumDates = len(DailyBusMinMax.Date)\n",
    "    \n",
    "    #this vectorized calculation follows the formula dailyDelta * randomVar + firstBusArrival to choose a random time\n",
    "        # between the EarliestBusArrival and the LatestBusArrival.\n",
    "        # this is done for every date and for each passenger in NumPassengers\n",
    "    pSim = pd.DataFrame(np.random.uniform(0,1,(NumDates,NumPassengers)))\n",
    "    pSim = pSim.mul(dailyDelta,axis=0)\n",
    "    pSim = pSim.add(dailyMin,axis=0)\n",
    "    \n",
    "    # add a dates column to front of dataframe  \n",
    "    pSim.insert(loc=0, column='Date', value=DailyBusMinMax['Date'])\n",
    "    \n",
    "    return pSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = passengerSim(DailyBusMinMax,500)\n",
    "sim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we reorganize the results of the passenger simulation to get a table that has one single `passengerId` column, instead of one column for each passenger. This is will allow us to perform a merge in the following step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = sim.melt(id_vars='Date')\n",
    "sim = sim.rename(columns={'variable':'passengerId','value':'passengerArrivalTime'})\n",
    "sim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As promised, we now merge the passenger simulation,`sim` with the bus arrival times, `BusArrivals`. The result of the code below is a lookup table where each passenger arrival time is associated with one bus arrival time; that is to say, the passengers are associated with the bus they board (which will always be the next bus that approaches the stop after they arrive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The powerset of all passenger-bus combinations\n",
    "busBoarding = pd.merge(right=sim, left=BusArrivals, on='Date', how='inner')\n",
    "\n",
    "# whittle down previous dataframe to those where passengers board busses that arrive at stop after they do\n",
    "    # (no going back in time)\n",
    "busBoarding = busBoarding.loc[busBoarding['BusArrivalTime']>=busBoarding['passengerArrivalTime']]\n",
    "\n",
    "# the passenger is reasonable and will board the first bus that approaches the stop\n",
    "busBoarding = busBoarding.groupby(['Date','passengerId','passengerArrivalTime']).first()\n",
    "\n",
    "# we reset the index to group by a different column in the next step\n",
    "busBoarding = busBoarding.reset_index().sort_values(['Date','BusArrivalTime','passengerId'])\n",
    "busBoarding.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we calcuate the number of people per bus by grouping by `BusArrivalTime` and then counting the number of entries in each group. We then merge this `busCrowding` DataFrame back into our `busBoarding` DataFrame from the previous step to give us a `numPassengersPerBus` column, which tells us exactly how many of our simulated passengers boarded each bus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busCrowding = busBoarding.groupby(['BusArrivalTime']).count()\n",
    "busCrowding = pd.DataFrame(busCrowding['passengerId']).rename(columns={'passengerId':'numPassengersPerBus'})\n",
    "busCrowding.reset_index()\n",
    "\n",
    "busBoarding = pd.merge(left=busBoarding, right=busCrowding, on='BusArrivalTime', how='inner')\n",
    "busBoarding.head()\n",
    "busBoarding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take a few columns of the `busBoarding`data that to plot the relationship between `passengerArrivalTime` and `numPassengersPerBus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData = busBoarding.loc[:,['passengerArrivalTime','numPassengersPerBus']]\n",
    "plotData['passengerArrivalTime'] = plotData.passengerArrivalTime.dt.time\n",
    "plotData.head()\n",
    "plotData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have our plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData = plotData.sort_values('passengerArrivalTime', ascending=True)\n",
    "_= plt.plot(plotData['passengerArrivalTime'], plotData['numPassengersPerBus'],'.',linestyle='none')\n",
    "_ = plt.xlabel('Passenger Arrival Time')\n",
    "_ = plt.ylabel('Number of Passengers on Bus')\n",
    "_ = plt.margins(0.02) # Keeps data off plot edges\n",
    "plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the above plot features a series of points a the 1000 people mark. This signifies that there are busses so crowded that they fit 1000 passengers inside. Clearly there is an innacuracy in the data. The source of the problem is that there is only one bus per day on some days, but the daily population of passengers remains 1000 every day. So far as we can see, there can be two causes for this error: \n",
    "\n",
    "1. Some of the bus arrival data has been unnecessarily deleted or is missing; there are no \"one-bus-only\" days at this stop.\n",
    "2. The number of passengers should be adapted to the number of busses for each transit day. This would mean the error was setting the daily ridership at a constant of `numPassengers=1000`.\n",
    "\n",
    "However, once find the source of this error, we will have our \"base truth\" to help train our models on in the next phase of our project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sam Mundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the elapsed time between passenger arrival and bus arrival (Wait time)\n",
    "busBoarding['WaitTime'] = busBoarding['BusArrivalTime'] - busBoarding['passengerArrivalTime']\n",
    "busBoarding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waitPlotData = busBoarding[['passengerArrivalTime', 'WaitTime']].copy()\n",
    "waitPlotData['passengerArrivalTime'] = waitPlotData.passengerArrivalTime.dt.time\n",
    "waitPlotData.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "waitPlotData = waitPlotData.sort_values('passengerArrivalTime', ascending=True)[0:500]\n",
    "_= plt.plot(waitPlotData['passengerArrivalTime'], waitPlotData['WaitTime'],'.',linestyle='none')\n",
    "_ = plt.xlabel('Passenger Arrival Time')\n",
    "_ = plt.ylabel('Wait Time For Bus')\n",
    "_ = plt.margins(0.02) # Keeps data off plot edges\n",
    "plt.xticks(rotation='vertical')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import relevant libraries and generate a dataframe representing all of the arrivals at a single stop in a single month traveling in one of two directions on the M100 bus route. \n",
    "\n",
    "It should be noted that there are relatively few entries for the month (107) due to data loss earlier in the cleaning process. Further work on cleaning is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "mta_df = pd.read_csv('../data/M100_month_W125_st.csv', error_bad_lines=False)\n",
    "mta_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the dataframe is sorted based on 'RecordedAtTime' which, since all entries are while the bus has reached the stop, represent the actual arrival times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_df['RecordedAtTime'] = pd.to_datetime(mta_df['RecordedAtTime'])\n",
    "mta_df.sort_values(\"RecordedAtTime\", inplace=True)\n",
    "mta_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arrivals dataframe is initialized with a controlled number of passenger arrival time entries and can have the frequency of random times changed in its definition statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_dates(frequency, NumDataPoints):\n",
    "    date_range = pd.date_range(start='2017-08-01', end='2017-08-30', freq=frequency)\n",
    "    random_dates = pd.to_datetime(\n",
    "        np.concatenate([\n",
    "                np.random.choice(date_range[1:-1], size=NumDataPoints, replace=False)\n",
    "            ])\n",
    "        )\n",
    "    return random_dates\n",
    "\n",
    "arrivals_df = pd.DataFrame()\n",
    "arrivals_df['PassengerTime'] = select_random_dates('1min', 600)\n",
    "arrivals_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next arriving bus is found for each of the random passenger arrival times defined above as well as the time delta between the two, representing wait time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNextBus(arrivals_df, mta_df):\n",
    "    for arrivalIndex, arrivalRow in arrivals_df.iterrows():\n",
    "        for mtaIndex, mtaRow in mta_df.iterrows():\n",
    "            if (mtaRow['RecordedAtTime'] > arrivalRow[0]):\n",
    "                arrivals_df.loc[arrivalIndex,'NextBus'] = mtaRow['RecordedAtTime']\n",
    "                break\n",
    "\n",
    "findNextBus(arrivals_df, mta_df)\n",
    "arrivals_df['WaitTime'] = arrivals_df['NextBus'] - arrivals_df['PassengerTime']\n",
    "arrivals_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Angelika Shastapalava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/1401011_weather_data.csv\", delimiter = ',', usecols = (5,24), index_col='DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selecting time period of aug 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = df.loc['2017-08-01' : '2017-09-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our data contains some na values. in addition some precip values contain \n",
    "letter 'T' or 's'.\n",
    "droping na and precip values that contain T or s letter in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = weather.dropna()\n",
    "weather = weather[~weather.HOURLYPrecip.str.contains(\"T\")]\n",
    "weather = weather[~weather.HOURLYPrecip.str.contains(\"s\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.HOURLYPrecip=pd.to_numeric(weather.HOURLYPrecip)\n",
    "weather.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn; seaborn.set()\n",
    "weather.plot(y='HOURLYPrecip', use_index=True)\n",
    "plt.xlabel('August 2017')\n",
    "plt.ylabel('Hourly Precipitation in Inches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating another df for visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pd.read_csv(\"../data/1401011_weather_data.csv\", delimiter = ',', usecols = (5,8),index_col='DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selecting time period of aug 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visib = vis.loc['2017-08-01' : '2017-09-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visib.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "droping na values and data that contain string 'V' in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visib = visib.dropna()\n",
    "visib = visib[~visib.HOURLYVISIBILITY.str.contains(\"V\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visib.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting visibility column to numeric in order to create a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visib.HOURLYVISIBILITY=pd.to_numeric(visib.HOURLYVISIBILITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visib.plot(y='HOURLYVISIBILITY', use_index=True)\n",
    "plt.xlabel('August 2017')\n",
    "plt.ylabel('Hourly Visibility in miles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what dates hourly visibility was below 1 mile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visib.loc[visib.HOURLYVISIBILITY < 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what dates horly precipitation was above 0.5 inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.loc[weather.HOURLYPrecip >= 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like on august 18th precipitation was relatively high and visibility was low\n",
    "This is important because it can influence waiting time and crowdiness in busses.\n",
    "For better understanding lets visualize it with seaborn plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.plot(y='HOURLYPrecip', use_index=True)\n",
    "plt.xlabel('August 2017')\n",
    "plt.ylabel('Hourly Precipitation in Inches')\n",
    "\n",
    "visib.plot(y='HOURLYVISIBILITY', use_index=True)\n",
    "plt.xlabel('August 2017')\n",
    "plt.ylabel('Hourly Visibility in miles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we have low hourly visibility and high precipitation \n",
    "around the same time (aug 18th).\n",
    "We can expect this date to have longer waiting time and \n",
    "higher crowdiness in busses in our future model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there any other dates that can influence our \n",
    "dependable variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(visib.loc[visib.HOURLYVISIBILITY < 2])\n",
    "print(weather.loc[weather.HOURLYPrecip >= 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like on aug 15th and 22nd there is \n",
    "a hight chance of finding disruption of bus services as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find out if there were any dates with hight wind speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pd.read_csv(\"../data/1401011_weather_data.csv\", delimiter = ',', usecols = (5,17), index_col='DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w.shape)\n",
    "print(len(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting time period of aug 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind = w.loc['2017-08-01' : '2017-09-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind = wind.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind.HOURLYWindSpeed.describe().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind.plot(y='HOURLYWindSpeed', use_index=True)\n",
    "plt.xlabel('August 2017')\n",
    "plt.ylabel('Hourly wind speed in miles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot it looks like highest wind speed occured\n",
    "towards the end of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind.loc[wind.HOURLYWindSpeed > 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed our observation was correct.\n",
    "However, we know that on aug 15th, 18th and 22nd \n",
    "our other 2 independant variables were relatively high/low.\n",
    "Higher than usual wind speed during those days would \n",
    "probably influence our dependent varibles even more.\n",
    "Let's see what wind speed we had on aug 15th, 18th and 22nd \n",
    "but before that let's convert index to datetime so we can use it later for \n",
    "creating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind = wind.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind.DATE = pd.to_datetime(wind.DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wind.loc[(wind.DATE >= '2017-08-15') & (wind.DATE <= '2017-08-16') ].max())\n",
    "print(wind.loc[(wind.DATE >= '2017-08-18') & (wind.DATE <= '2017-08-19') ].max())\n",
    "print(wind.loc[(wind.DATE >= '2017-08-22') & (wind.DATE <= '2017-08-23') ].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see max wind speed on aug 18th and 22ns was \n",
    "8 and 11 mph respectively\n",
    "this also can influence out predictors.\n",
    "We need to convert DATE to datetime for precipitation and visibility as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = weather.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.DATE = pd.to_datetime(weather.DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visib = visib.reset_index()\n",
    "visib.DATE = pd.to_datetime(visib.DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visib.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finished with cleaning and analyzing weather data.\n",
    "Our observations show that there are certain days when we can expect higher crowdiness \n",
    "and longer wait time for the busses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we ready to integrate the data to our model as independant variables and see\n",
    "if our predictions were correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel Espina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random_state = 20181112\n",
    "import datetime, math, glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding data from the M100 csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "df = pd.read_csv('../data/M100_month_W125_st.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the Best Classifier\n",
    "\n",
    "We want (a) regressor(s) that can predict the **wait time** and **crowding** of a bus at a specific stop with the inputs **hourly weather** and **time of day**. We would most likely have two models that predict each **wait time** and **crowding**.\n",
    "\n",
    "Here are our top picks for regressors:\n",
    "\n",
    "1. Gradient Boosting Machines ***(top pick)***:\n",
    "    - Why: GBMs are typically a composite model that combines the efforts of multiple weak models to create a strong model, and each additional weak model reduces the mean squared error (MSE) of the overall model. Our goal would be to minimize MSE to increase the accuracy of our predictions.\n",
    "\n",
    "1. Random Forest:\n",
    "    - Why: does not suffer from the overfitting like with Decision Trees. Instead of randomly choosing to split from just **hourly weather** and **time of day**, we can have two trees that randomly split from each and find the best model. \n",
    "\n",
    "1. Decision Trees:  \n",
    "    - Reduction in Standard Deviation (metric): This is a regression metric that measures how much we’ve reduced our uncertainty by picking a split point. By picking the best split each time the greedy decision tree training algorithm tries to form decisions with as few splits as possible.  \n",
    "    - Hyperparameters:   \n",
    "        * Max depth: Limit our tree to a `n` depth to prevent overfitting.\n",
    "        \n",
    "\n",
    "Evaluating our model:\n",
    "\n",
    "Since we're creating regression models, we are interested in the ***mean squared error*** and ***R Squared***. The lower our ***R Squared*** the more accurate our model. We intend to use **K-fold cross validation** as well as a **holdout set** as we improve our model through hyperparameter tuning. \n",
    "\n",
    "    * Preventing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "What we need to do:  \n",
    "\n",
    "1. Clean and break up the time components (Hour, Mins, Secs) of the following:\n",
    "    * `RecordedAtTime`\n",
    "    * `ExpectedArrivalTime`\n",
    "    * `ScheduledArrivalTime`\n",
    "    \n",
    "2. Store features of interest:\n",
    "    * `RecordedAtTime`\n",
    "    * `VehicleLocation.Longitude`\n",
    "    * `VehicleLocation.Latitude`\n",
    "    * `DistanceFromStop`\n",
    "    * `ExpectedArrivalTime`\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ScheduledArrivalTime'] = pd.to_datetime(df.ScheduledArrivalTime, errors='coerce')\n",
    "df.dropna()\n",
    "df['Scheduled_Hour'] = df['ScheduledArrivalTime'].dt.hour\n",
    "df['Scheduled_Minute'] = df['ScheduledArrivalTime'].dt.minute\n",
    "df['Scheduled_Seconds'] = df['ScheduledArrivalTime'].dt.second\n",
    "\n",
    "df['RecordedAtTime'] = pd.to_datetime(df.RecordedAtTime)\n",
    "df['Recorded_Hour'] = pd.to_datetime(df.RecordedAtTime).dt.hour\n",
    "df['Recorded_Minute'] = pd.to_datetime(df.RecordedAtTime).dt.minute\n",
    "df['Recorded_Seconds'] = pd.to_datetime(df.RecordedAtTime).dt.second\n",
    "\n",
    "df['ExpectedArrivalTime'] = pd.to_datetime(df.ExpectedArrivalTime)\n",
    "df['Expected_Hour'] = pd.to_datetime(df.ExpectedArrivalTime).dt.hour\n",
    "df['Expected_Minute'] = pd.to_datetime(df.ExpectedArrivalTime).dt.minute\n",
    "df['Expected_Seconds'] = pd.to_datetime(df.ExpectedArrivalTime).dt.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = (['VehicleLocation.Longitude', \n",
    "             'VehicleLocation.Latitude', \n",
    "             'OriginLong',\n",
    "             'OriginLat',\n",
    "             'DistanceFromStop',\n",
    "             'Recorded_Hour',\n",
    "             'Scheduled_Hour',\n",
    "             'Scheduled_Minute',\n",
    "             'Scheduled_Seconds',\n",
    "             'Recorded_Minute',\n",
    "             'Recorded_Seconds',\n",
    "             'time_diff_bus_mins'\n",
    "            ])\n",
    "\n",
    "model_df = df[(features)].dropna().reset_index()\n",
    "\n",
    "model_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting a Chart for Sanity\n",
    "\n",
    "We want to have a frequency/histogram for each hour of the day and for each minute of the hour.\n",
    "\n",
    "Credit: David"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(inputSeries, label):\n",
    "    try:\n",
    "        x = np.sort(inputSeries)\n",
    "    except:\n",
    "        print(\"Warning: Series Unsorted\")\n",
    "        x = inputSeries\n",
    "    y = np.arange(1, len(x)+1) / len(x)\n",
    "    _ = plt.plot(x, y, marker='.', linestyle='none')\n",
    "    _ = plt.xlabel('Time Delta ({})'.format(label))\n",
    "    _ = plt.ylabel('ECDF')\n",
    "    plt.margins(0.02) # Keeps data off plot edges\n",
    "    plt.show()\n",
    "    \n",
    "def hist(inputSeries, label):\n",
    "    plt.hist(inputSeries, bins=25, density=True)\n",
    "    _ = plt.xlabel('Time Delta ({})'.format(label))\n",
    "    _ = plt.ylabel('PDF')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M100_NICK_Avg = df[['Recorded_Hour','time_diff_bus_mins', 'Recorded_Minute']]\n",
    "M100_Hour = M100_NICK_Avg.groupby('Recorded_Hour').mean().dropna()\n",
    "M100_Min = M100_NICK_Avg.groupby('Recorded_Minute').mean().dropna()\n",
    "M100_NICK_Avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ecdf(M100_Hour['time_diff_bus_mins'], \"per Hour\")\n",
    "hist(M100_Hour['time_diff_bus_mins'], \"per Hour\")\n",
    "\n",
    "ecdf(M100_Min['time_diff_bus_mins'], \"per Minute\")\n",
    "hist(M100_Min['time_diff_bus_mins'], \"per Minute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving our Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.to_csv('M100_4_month_W125_st_timesplit.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, holdout_df, y_train, y_holdout = train_test_split(model_df[features],\n",
    "                                                    model_df['time_diff_bus_mins'],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)\n",
    "\n",
    "train_df['time_diff_bus_mins'] = y_train\n",
    "holdout_df['time_diff_bus_mins'] = y_holdout\n",
    "\n",
    "train_df.reset_index(inplace=True)\n",
    "holdout_df.reset_index(inplace=True)\n",
    "\n",
    "print(train_df.shape[0], train_df.time_diff_bus_mins.mean())\n",
    "print(holdout_df.shape[0], holdout_df.time_diff_bus_mins.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Let's take a quick look at all of our classification model options using cross validation. For the tree based models, we'll use the hyperparameter `max_depth=6` as a naive attempt at voiding overfitting before we dig deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit and score the model, this time using cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=5, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_results(classifier):\n",
    "    \n",
    "    results = []\n",
    "    for train, test in k_fold.split(train_df):\n",
    "        classifier.fit(train_df.loc[train, features], train_df.loc[train, 'time_diff_bus_mins'])\n",
    "        y_predicted = classifier.predict(train_df.loc[test, features])\n",
    "        accuracy = accuracy_score(train_df.loc[test, 'time_diff_bus_mins'], y_predicted)\n",
    "        results.append(accuracy)\n",
    "    \n",
    "    return np.mean(results), np.std(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(\n",
    "    random_state=random_state, \n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "get_cv_results(logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(\n",
    "    random_state=random_state, \n",
    "    max_depth=6\n",
    ")\n",
    "\n",
    "get_cv_results(dtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rforest = RandomForestClassifier(\n",
    "    random_state=random_state, \n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "get_cv_results(rforest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingClassifier(\n",
    "    random_state=random_state, \n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "get_cv_results(gbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance\n",
    "\n",
    "We're using ROC curves to visually see which model performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(classifier, label, color):\n",
    "\n",
    "    classifier.fit(train_df[features], train_df['time_diff_bus_mins'])\n",
    "    y_prob = classifier.predict_proba(holdout_df[features])\n",
    "    \n",
    "    fpr, tpr, thresh = roc_curve(holdout_df['time_diff_bus_mins'], y_prob[:,1])\n",
    "    plt.plot(fpr, tpr,\n",
    "             label=label,\n",
    "             color=color, linewidth=3)\n",
    "\n",
    "    auc = roc_auc_score(holdout_df['time_diff_bus_mins'], y_prob[:,1])\n",
    "    \n",
    "    print('AUC: %0.3f (%s)' % (auc, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = plt.figure(figsize=(14,6))\n",
    "\n",
    "logreg = LogisticRegression(\n",
    "    random_state=random_state, \n",
    "    solver='lbfgs'\n",
    ")\n",
    "plot_roc(logreg, 'Logistic Regression', 'green')\n",
    "\n",
    "dtree = DecisionTreeClassifier(\n",
    "    random_state=random_state, \n",
    "    max_depth=3\n",
    ")\n",
    "plot_roc(dtree, 'Decision Tree', 'red')\n",
    "\n",
    "rforest = RandomForestClassifier(\n",
    "    random_state=random_state, \n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "plot_roc(rforest, 'Random Forest', 'blue')\n",
    "\n",
    "gbm = GradientBoostingClassifier(\n",
    "    random_state=random_state, \n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "plot_roc(gbm, 'GBM', 'lightblue')\n",
    "\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
