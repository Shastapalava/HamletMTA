{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling M100 Lateness\n",
    "\n",
    "Here we're trying to model the M100's lateness and simulated crowdedness in the St. Nicholas stop going to Inwood 220 St Via Amsterdam Via Bway. \n",
    "\n",
    "We are applying Datacamp's Decision-Tree for Classification\n",
    "\n",
    "## Table of Contents:\n",
    "1. [Choosing the Appropriate Classifier](#choosing-the-appropriate-classifier)\n",
    "1. [Plotting a Chart for Sanity](#plotting-a-chart-for-sanity)\n",
    "1. [Saving our Progress](#saving-our-progress)\n",
    "1. [Model Training](#model-training)\\*\n",
    "1. [Data Cleaning](#data-cleaning)\\*\n",
    "\n",
    "\\* Not finished yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prequisites (if you want to follow along/verify results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading https://files.pythonhosted.org/packages/a8/76/220ba4420459d9c4c9c9587c6ce607bf56c25b3d3d2de62056efe482dadc/seaborn-0.9.0-py3-none-any.whl (208kB)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14.0 in c:\\users\\win10\\anaconda3\\lib\\site-packages (from seaborn) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.3 in c:\\users\\win10\\anaconda3\\lib\\site-packages (from seaborn) (1.15.3)\n",
      "Requirement already satisfied, skipping upgrade: pandas>=0.15.2 in c:\\users\\win10\\anaconda3\\lib\\site-packages (from seaborn) (0.23.4)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib>=1.4.3 in c:\\users\\win10\\anaconda3\\lib\\site-packages (from seaborn) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in c:\\users\\win10\\anaconda3\\lib\\site-packages (from pandas>=0.15.2->seaborn) (2.7.3)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in c:\\users\\win10\\anaconda3\\lib\\site-packages (from pandas>=0.15.2->seaborn) (2018.5)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in c:\\users\\win10\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in c:\\users\\win10\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->seaborn) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\win10\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->seaborn) (2.2.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in c:\\users\\win10\\anaconda3\\lib\\site-packages (from python-dateutil>=2.5.0->pandas>=0.15.2->seaborn) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\users\\win10\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (39.1.0)\n",
      "Installing collected packages: seaborn\n",
      "  Found existing installation: seaborn 0.8\n",
      "    Uninstalling seaborn-0.8:\n",
      "      Successfully uninstalled seaborn-0.8\n",
      "Successfully installed seaborn-0.9.0\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install --user -U scikit-learn==0.18\n",
    "!pip3 install --user -U seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import datetime as dt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random_state = 20181112\n",
    "import datetime, math, glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding data from the M100 csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the Appropriate Classifier\n",
    "\n",
    "We want (a) regressor(s) that can predict the **wait time** and **crowding** of a bus at a specific stop with the inputs **hourly weather** and **time of day**. We would most likely have two models that predict each **wait time** and **crowding**.\n",
    "\n",
    "Here are our top picks for regressors:\n",
    "\n",
    "1. Gradient Boosting Machines ***(top pick)***:\n",
    "    - Why: GBMs are typically a composite model that combines the efforts of multiple weak models to create a strong model, and each additional weak model reduces the mean squared error (MSE) of the overall model. Our goal would be to minimize MSE to increase the accuracy of our predictions.\n",
    "\n",
    "1. Random Forest:\n",
    "    - Why: does not suffer from the overfitting like with Decision Trees. Instead of randomly choosing to split from just **hourly weather** and **time of day**, we can have two trees that randomly split from each and find the best model. \n",
    "\n",
    "1. Decision Trees:  \n",
    "    - Reduction in Standard Deviation (metric): This is a regression metric that measures how much weâ€™ve reduced our uncertainty by picking a split point. By picking the best split each time the greedy decision tree training algorithm tries to form decisions with as few splits as possible.  \n",
    "    - Hyperparameters:   \n",
    "        * Max depth: Limit our tree to a `n` depth to prevent overfitting.\n",
    "        \n",
    "\n",
    "Evaluating our model:\n",
    "\n",
    "Since we're creating regression models, we are interested in the ***mean squared error*** and ***R Squared***. The lower our ***R Squared*** the more accurate our model. We intend to use **K-fold cross validation** as well as a **holdout set** as we improve our model through hyperparameter tuning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "> Please checkout [this notebook](../Bus_Timeline/Excel_Bus_Timeline_Draft.ipynb) on how we did the cleaning process\n",
    "\n",
    "1. Clean and break up the time components (Hour, Mins, Secs) of the following:\n",
    "    * `RecordedAtTime`\n",
    "2. Merge and store (we'll merge them based on the hour of the day and the day of the month):\n",
    "    * Bus\n",
    "        * `Hour`\n",
    "        * `Min`\n",
    "        * `Sec`\n",
    "        * `Day`\n",
    "    * Weather\n",
    "        * `Hour`\n",
    "        * `HourlyVisibility`\n",
    "        * `HourlyPrecipitation`\n",
    "        * `HourlyWindSpeed`\n",
    "3. Features of interest:\n",
    "    * `Hour`\n",
    "    * `Min`\n",
    "    * `Sec`\n",
    "    * `HourlyVisibility`\n",
    "    * `HourlyPrecipitation`\n",
    "    * `HourlyWindSpeed`\n",
    "4. Prediction result:\n",
    "    * `timeTillNext`: estimated minutes remaining until next bus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our Merged Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Merged_Bus_Weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training I\n",
    "\n",
    "Adapted from: https://shankarmsy.github.io/stories/gbrt-sklearn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings, seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "random_state = 42\n",
    "\n",
    "np.random.seed(sum(map(ord, \"aesthetics\"))) \n",
    "seaborn.set_context('notebook') \n",
    "# pd.set_option('display.mpl_style', 'default') # Make the graphs a bit prettier \n",
    "plt.rcParams['figure.figsize'] = (15, 5) # Set some Pandas options \n",
    "pd.set_option('display.notebook_repr_html', False) \n",
    "pd.set_option('display.max_columns', 40) \n",
    "pd.set_option('display.max_rows', 25) \n",
    "pd.options.display.max_colwidth = 50 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features, Targets and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "passengerArrivalTime     object\n",
       "numPassengersPerBus       int64\n",
       "BusDepartureTime         object\n",
       "HOURLYVISIBILITY        float64\n",
       "HOURLYWindSpeed         float64\n",
       "HOURLYPrecip            float64\n",
       "ArrivalHour               int64\n",
       "ArrivalSeconds            int64\n",
       "ArrivalMinutes            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11547 26.82999913397419\n",
      "2887 27.04052649809491\n"
     ]
    }
   ],
   "source": [
    "features = (['ArrivalHour', 'ArrivalSeconds', 'ArrivalMinutes', \n",
    "             'HOURLYVISIBILITY', 'HOURLYWindSpeed', 'HOURLYPrecip'])\n",
    "\n",
    "target = 'numPassengersPerBus'\n",
    "\n",
    "model_df = df[(features + [target])].dropna().reset_index()\n",
    "\n",
    "train_df, holdout_df, y_train, y_holdout = train_test_split(\n",
    "    model_df[features], \n",
    "    model_df[target], test_size=0.2,\n",
    "    random_state=random_state)\n",
    "\n",
    "train_df[target] = y_train\n",
    "holdout_df[target] = y_holdout\n",
    "\n",
    "train_df.reset_index(inplace=True)\n",
    "holdout_df.reset_index(inplace=True)\n",
    "\n",
    "print(train_df.shape[0], train_df.numPassengersPerBus.mean())\n",
    "print(holdout_df.shape[0], holdout_df.numPassengersPerBus.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df\n",
    "X_test = holdout_df\n",
    "y_train = y_train\n",
    "y_test = y_holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gradient Boosting Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt=GradientBoostingRegressor(n_estimators=100) \n",
    "train_df.shape\n",
    "\n",
    "gbrt.fit(train_df, y_train) \n",
    "y_pred=gbrt.predict(holdout_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=10, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_results(regressor):\n",
    "    \n",
    "    mse = []\n",
    "    for train, test in k_fold.split(train_df):\n",
    "        regressor.fit(train_df.loc[train, features], train_df.loc[train, target])\n",
    "        y_predicted = regressor.predict(train_df.loc[test, features])\n",
    "        \n",
    "        mean_squared = mean_squared_error(train_df.loc[test, target], y_predicted)\n",
    "        mse.append(mean_squared)\n",
    "    \n",
    "    return np.mean(mse), np.std(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of mean squared error: 361.38612937537414\n",
      "Mean squared error std: 36.11640817377122\n"
     ]
    }
   ],
   "source": [
    "gbm = GradientBoostingRegressor(\n",
    "    random_state=random_state, \n",
    "    learning_rate = 0.01,\n",
    "    min_samples_split=4,\n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "results = get_cv_results(gbm)\n",
    "\n",
    "print(\"Mean of mean squared error:\", results[0])\n",
    "print(\"Mean squared error std:\", results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we change some hyperparameters and see what the outcomes are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of mean squared error: 197.64028769727676\n",
      "Mean squared error std: 27.16784083705199\n"
     ]
    }
   ],
   "source": [
    "gbm = GradientBoostingRegressor(\n",
    "    random_state=random_state, \n",
    "    learning_rate = 0.05,\n",
    "    min_samples_split=4,\n",
    "    max_depth=4,\n",
    "    n_estimators=600\n",
    ")\n",
    "\n",
    "results = get_cv_results(gbm)\n",
    "\n",
    "print(\"Mean of mean squared error:\", results[0])\n",
    "print(\"Mean squared error std:\", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of mean squared error: 211.13588172527088\n",
      "Mean squared error std: 28.384793002154694\n"
     ]
    }
   ],
   "source": [
    "gbm = GradientBoostingRegressor(\n",
    "    random_state=random_state, \n",
    "    learning_rate = 0.01,\n",
    "    min_samples_split=100,\n",
    "    max_depth=4,\n",
    "    n_estimators=800\n",
    ")\n",
    "\n",
    "results = get_cv_results(gbm)\n",
    "\n",
    "print(\"Mean of mean squared error:\", results[0])\n",
    "print(\"Mean squared error std:\", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.1, 0.05, 0.01]\n",
    "min_samples_splits = range(20, 100, 20)\n",
    "max_depths = [4, 6, 8]\n",
    "n_estimators = range(200,1000,200)\n",
    "\n",
    "all_mu = []\n",
    "all_sigma = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    for min_splits in min_samples_splits:\n",
    "        for rate in learning_rates:\n",
    "            for est in n_estimators:\n",
    "                print(\"Depth:\", depth, \"Splits:\", min_splits, \"Rate:\", rate, \"n_estimators:\", est, end=\" \")\n",
    "                gbrm=GradientBoostingRegressor(\n",
    "                    random_state=random_state, \n",
    "                    max_depth=depth,\n",
    "                    min_samples_split=min_splits,\n",
    "                    learning_rate=rate,\n",
    "                    n_estimators=est\n",
    "                )\n",
    "\n",
    "                mu, sigma = get_cv_results(gbrm)\n",
    "                all_mu.append(mu)\n",
    "                all_sigma.append(sigma)\n",
    "\n",
    "                print(mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(hp_values, all_mu)\n",
    "plt.ylabel('Cross Validation Mean MSE')\n",
    "plt.xlabel('Max Depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(hp_values, all_sigma)\n",
    "plt.ylabel('Cross Validation Std Dev. of MSE')\n",
    "plt.xlabel('Max Depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(regressor, label, color):\n",
    "\n",
    "    regressor.fit(train_df[features], train_df[target])\n",
    "    y_prob = regressor.predict(holdout_df[features])\n",
    "    \n",
    "    fpr, tpr, thresh = roc_curve(holdout_df[target], y_prob)\n",
    "    plt.plot(fpr, tpr,\n",
    "             label=label,\n",
    "             color=color, linewidth=3)\n",
    "\n",
    "    auc = roc_auc_score(holdout_df[target], y_prob[:,1])\n",
    "    \n",
    "    print('AUC: %0.3f (%s)' % (auc, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = plt.figure(figsize=(14,6))\n",
    "gbm = GradientBoostingRegressor(\n",
    "    random_state=random_state, \n",
    "    learning_rate = 0.01,\n",
    "    min_samples_split=4,\n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "plot_roc(gbm, 'GBM', 'lightblue')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving/Loading the Model\n",
    "\n",
    "\n",
    "Credit: https://scikit-learn.org/stable/modules/model_persistence.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U --user joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving\n",
    "from joblib import dump, load\n",
    "dump(estimator, '../../data/GBRT_Hamlet.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading\n",
    "model = load('../../data/GBRT_Hamlet.joblib') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
