{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling M100 Lateness\n",
    "\n",
    "Here we're trying to model the M100's lateness and simulated crowdedness in the St. Nicholas stop going to Inwood 220 St Via Amsterdam Via Bway. \n",
    "\n",
    "We are applying Datacamp's Decision-Tree for Classification\n",
    "\n",
    "## Table of Contents:\n",
    "1. [Data Cleaning](#data-cleaning)\n",
    "1. [Plotting a Chart for Sanity](#plotting-a-chart-for-sanity)\n",
    "1. [Saving our Progress](#saving-our-progress)\n",
    "1. [Model Training](#model-training)\\*\n",
    "1. [Data Cleaning](#data-cleaning)\\*\n",
    "\n",
    "\\* Not finished yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random_state = 20181112\n",
    "import datetime, math, glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding data from the M100 csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the Best Classifier\n",
    "\n",
    "We want (a) regressor(s) that can predict the **wait time** and **crowding** of a bus at a specific stop with the inputs **hourly weather** and **time of day**. We would most likely have two models that predict each **wait time** and **crowding**.\n",
    "\n",
    "Here are our top picks for regressors:\n",
    "\n",
    "1. Gradient Boosting Machines ***(top pick)***:\n",
    "    - Why: GBMs are typically a composite model that combines the efforts of multiple weak models to create a strong model, and each additional weak model reduces the mean squared error (MSE) of the overall model. Our goal would be to minimize MSE to increase the accuracy of our predictions.\n",
    "\n",
    "1. Random Forest:\n",
    "    - Why: does not suffer from the overfitting like with Decision Trees. Instead of randomly choosing to split from just **hourly weather** and **time of day**, we can have two trees that randomly split from each and find the best model. \n",
    "\n",
    "1. Decision Trees:  \n",
    "    - Reduction in Standard Deviation (metric): This is a regression metric that measures how much weâ€™ve reduced our uncertainty by picking a split point. By picking the best split each time the greedy decision tree training algorithm tries to form decisions with as few splits as possible.  \n",
    "    - Hyperparameters:   \n",
    "        * Max depth: Limit our tree to a `n` depth to prevent overfitting.\n",
    "        \n",
    "\n",
    "Evaluating our model:\n",
    "\n",
    "Since we're creating regression models, we are interested in the ***mean squared error*** and ***R Squared***. The lower our ***R Squared*** the more accurate our model. We intend to use **K-fold cross validation** as well as a **holdout set** as we improve our model through hyperparameter tuning. \n",
    "\n",
    "    * Preventing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "What we need to do:  \n",
    "\n",
    "1. Clean and break up the time components (Hour, Mins, Secs) of the following:\n",
    "    * `RecordedAtTime`\n",
    "2. Merge and store (we'll merge them based on the hour of the day and the day of the month):\n",
    "    * Bus\n",
    "        * `Hour`\n",
    "        * `Min`\n",
    "        * `Sec`\n",
    "        * `Day`\n",
    "    * Weather\n",
    "        * `Hour`\n",
    "        * `HourlyVisibility`\n",
    "        * `HourlyPrecipitation`\n",
    "        * `HourlyWindSpeed`\n",
    "3. Features of interest:\n",
    "    * `Hour`\n",
    "    * `Min`\n",
    "    * `Sec`\n",
    "    * `Hour`\n",
    "    * `HourlyVisibility`\n",
    "    * `HourlyPrecipitation`\n",
    "    * `HourlyWindSpeed`\n",
    "4. Prediction result:\n",
    "    * `Hour`\n",
    "    * `Min`\n",
    "    * `Sec`\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving our Progress/Merging Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "weather = pd.read_csv('../../data/1401011_weather_data.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "m100 = pd.read_csv('../../data/M100_Features.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Unnamed: 0      RecordedAtTime VehicleRef        timeDelta  Day  Hour  \\\n",
       "5393        5393 2017-08-29 19:32:56  NYCT_8363  0 days 00:09:59   29    19   \n",
       "5394        5394 2017-08-29 19:32:59  NYCT_8389  0 days 00:10:28   29    19   \n",
       "5395        5395 2017-08-29 19:42:01  NYCT_4375  0 days 02:50:12   29    19   \n",
       "5396        5396 2017-08-29 19:42:01  NYCT_4375  0 days 00:00:00   29    19   \n",
       "5397        5397 2017-08-29 19:42:32  NYCT_8375  0 days 02:20:49   29    19   \n",
       "\n",
       "      Min  Sec     stringTime  \n",
       "5393   32   56  2017-08-29 19  \n",
       "5394   32   59  2017-08-29 19  \n",
       "5395   42    1  2017-08-29 19  \n",
       "5396   42    1  2017-08-29 19  \n",
       "5397   42   32  2017-08-29 19  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m100.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Credit: Angelika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newWeather = weather[['DATE','HOURLYVISIBILITY', 'HOURLYWindSpeed', 'HOURLYPrecip']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newWeather = newWeather.dropna()\n",
    "newWeather = newWeather[~newWeather.HOURLYPrecip.str.contains(\"T\")]\n",
    "newWeather = newWeather[~newWeather.HOURLYPrecip.str.contains(\"s\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "newWeather = newWeather[~newWeather.HOURLYVISIBILITY.str.contains(\"V\")]\n",
    "newWeather.HOURLYVISIBILITY=pd.to_numeric(newWeather.HOURLYVISIBILITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    @params:\n",
    "        weather: weather table\n",
    "        bus: bus table\n",
    "''' \n",
    "\n",
    "def mergeOnDateTime(bus, weather):\n",
    "    weather['DATE'] = pd.to_datetime(weather['DATE'])\n",
    "    bus['RecordedAtTime'] = pd.to_datetime(bus['RecordedAtTime'])\n",
    "    \n",
    "    weather['stringTime'] = weather['DATE'].apply(lambda x: x.strftime('%Y-%m-%d %H'))\n",
    "    bus['stringTime'] = bus['RecordedAtTime'].apply(lambda x: x.strftime('%Y-%m-%d %H'))\n",
    "    \n",
    "    newTable = pd.merge(left=bus, right=weather,  how='inner', on=['stringTime'])\n",
    "    \n",
    "    newTable.drop(columns='stringTime', inplace=True, axis=1)\n",
    "    \n",
    "    return newTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# newWeather['Hour'] = pd.to_datetime(weather['DATE']).dt.hour\n",
    "# newWeather['Day'] = pd.to_datetime(weather['DATE']).dt.day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Time gate to August\n",
    "\n",
    "newWeather = newWeather[(newWeather['DATE'] > '2017-08-01') & (newWeather['DATE'] < '2017-09-01')].reset_index().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['index'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-174-da0a6981b34f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# newWeather = newWeather[(newWeather['HOUR'] > 4) & (newWeather['HOUR'] < 20)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnewWeather\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mnewWeather\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mnewWeather\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3695\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3696\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3697\u001b[1;33m                                            errors=errors)\n\u001b[0m\u001b[0;32m   3698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3699\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3109\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3111\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3141\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3142\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3143\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3144\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   4402\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4403\u001b[0m                 raise KeyError(\n\u001b[1;32m-> 4404\u001b[1;33m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[0;32m   4405\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4406\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['index'] not found in axis\""
     ]
    }
   ],
   "source": [
    "\n",
    "# Fix some data types\n",
    "newWeather['HOURLYPrecip'] = pd.to_numeric(newWeather['HOURLYPrecip'], downcast='float', errors='coerce')\n",
    "newWeather['HOURLYVISIBILITY'] = pd.to_numeric(newWeather['HOURLYVISIBILITY'], downcast='float', errors='coerce')\n",
    "# Bound hour of day\n",
    "# newWeather = newWeather[(newWeather['HOUR'] > 4) & (newWeather['HOUR'] < 20)]\n",
    "newWeather.reset_index()\n",
    "newWeather.drop(columns=['index'], inplace=True, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 DATE  HOURLYVISIBILITY  HOURLYWindSpeed  HOURLYPrecip\n",
       "0    2017-08-01 00:51              10.0              3.0           0.0\n",
       "1    2017-08-01 01:51              10.0              0.0           0.0\n",
       "2    2017-08-01 02:51              10.0              0.0           0.0\n",
       "3    2017-08-01 03:51              10.0              3.0           0.0\n",
       "4    2017-08-01 04:51              10.0              0.0           0.0\n",
       "5    2017-08-01 05:51              10.0              0.0           0.0\n",
       "6    2017-08-01 06:51              10.0              0.0           0.0\n",
       "7    2017-08-01 07:51              10.0              5.0           0.0\n",
       "8    2017-08-01 08:51              10.0              6.0           0.0\n",
       "9    2017-08-01 09:51              10.0              0.0           0.0\n",
       "10   2017-08-01 10:51              10.0              3.0           0.0\n",
       "11   2017-08-01 11:51              10.0              5.0           0.0\n",
       "..                ...               ...              ...           ...\n",
       "747  2017-08-31 12:51              10.0              7.0           0.0\n",
       "748  2017-08-31 13:51              10.0              3.0           0.0\n",
       "749  2017-08-31 14:51              10.0              6.0           0.0\n",
       "750  2017-08-31 15:51              10.0              0.0           0.0\n",
       "751  2017-08-31 16:51              10.0              7.0           0.0\n",
       "752  2017-08-31 17:51              10.0              3.0           0.0\n",
       "753  2017-08-31 18:51              10.0              7.0           0.0\n",
       "754  2017-08-31 19:51               9.0              0.0           0.0\n",
       "755  2017-08-31 20:51              10.0              3.0           0.0\n",
       "756  2017-08-31 21:51              10.0              0.0           0.0\n",
       "757  2017-08-31 22:51              10.0              6.0           0.0\n",
       "758  2017-08-31 23:51              10.0              3.0           0.0\n",
       "\n",
       "[759 rows x 4 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newWeather.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(759, 4)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newWeather.dtypes\n",
    "newWeather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5398, 9)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m100.dtypes\n",
    "m100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mergeOnDateTime(m100, newWeather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                   int64\n",
       "RecordedAtTime      datetime64[ns]\n",
       "VehicleRef                  object\n",
       "timeDelta                   object\n",
       "Day                          int64\n",
       "Hour                         int64\n",
       "Min                          int64\n",
       "Sec                          int64\n",
       "DATE                datetime64[ns]\n",
       "HOURLYVISIBILITY           float32\n",
       "HOURLYWindSpeed            float64\n",
       "HOURLYPrecip               float32\n",
       "dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0', 'timeDelta'], inplace=True, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      index      RecordedAtTime VehicleRef  Day  Hour  Min  Sec  \\\n",
       "0         0 2017-08-01 00:11:39  NYCT_4349    1     0   11   39   \n",
       "1         1 2017-08-01 00:21:06  NYCT_4368    1     0   21    6   \n",
       "2         2 2017-08-01 00:41:29  NYCT_8375    1     0   41   29   \n",
       "3         3 2017-08-01 00:51:52  NYCT_8375    1     0   51   52   \n",
       "4         4 2017-08-01 01:01:11  NYCT_8375    1     1    1   11   \n",
       "5         5 2017-08-01 05:21:36  NYCT_8368    1     5   21   36   \n",
       "6         6 2017-08-01 05:21:36  NYCT_8368    1     5   21   36   \n",
       "7         7 2017-08-01 05:21:36  NYCT_8368    1     5   21   36   \n",
       "8         8 2017-08-01 05:31:39  NYCT_8368    1     5   31   39   \n",
       "9         9 2017-08-01 05:31:39  NYCT_8368    1     5   31   39   \n",
       "10       10 2017-08-01 05:31:39  NYCT_8368    1     5   31   39   \n",
       "11       11 2017-08-01 06:01:49  NYCT_8368    1     6    1   49   \n",
       "...     ...                 ...        ...  ...   ...  ...  ...   \n",
       "5715   5715 2017-08-29 18:52:07  NYCT_8398   29    18   52    7   \n",
       "5716   5716 2017-08-29 19:02:05  NYCT_8373   29    19    2    5   \n",
       "5717   5717 2017-08-29 19:11:57  NYCT_8376   29    19   11   57   \n",
       "5718   5718 2017-08-29 19:12:06  NYCT_8389   29    19   12    6   \n",
       "5719   5719 2017-08-29 19:22:31  NYCT_8389   29    19   22   31   \n",
       "5720   5720 2017-08-29 19:22:57  NYCT_8363   29    19   22   57   \n",
       "5721   5721 2017-08-29 19:22:58  NYCT_8376   29    19   22   58   \n",
       "5722   5722 2017-08-29 19:32:56  NYCT_8363   29    19   32   56   \n",
       "5723   5723 2017-08-29 19:32:59  NYCT_8389   29    19   32   59   \n",
       "5724   5724 2017-08-29 19:42:01  NYCT_4375   29    19   42    1   \n",
       "5725   5725 2017-08-29 19:42:01  NYCT_4375   29    19   42    1   \n",
       "5726   5726 2017-08-29 19:42:32  NYCT_8375   29    19   42   32   \n",
       "\n",
       "                    DATE  HOURLYVISIBILITY  HOURLYWindSpeed  HOURLYPrecip  \n",
       "0    2017-08-01 00:51:00              10.0              3.0          0.00  \n",
       "1    2017-08-01 00:51:00              10.0              3.0          0.00  \n",
       "2    2017-08-01 00:51:00              10.0              3.0          0.00  \n",
       "3    2017-08-01 00:51:00              10.0              3.0          0.00  \n",
       "4    2017-08-01 01:51:00              10.0              0.0          0.00  \n",
       "5    2017-08-01 05:51:00              10.0              0.0          0.00  \n",
       "6    2017-08-01 05:51:00              10.0              0.0          0.00  \n",
       "7    2017-08-01 05:51:00              10.0              0.0          0.00  \n",
       "8    2017-08-01 05:51:00              10.0              0.0          0.00  \n",
       "9    2017-08-01 05:51:00              10.0              0.0          0.00  \n",
       "10   2017-08-01 05:51:00              10.0              0.0          0.00  \n",
       "11   2017-08-01 06:51:00              10.0              0.0          0.00  \n",
       "...                  ...               ...              ...           ...  \n",
       "5715 2017-08-29 18:51:00              10.0              9.0          0.01  \n",
       "5716 2017-08-29 19:51:00               8.0              8.0          0.02  \n",
       "5717 2017-08-29 19:51:00               8.0              8.0          0.02  \n",
       "5718 2017-08-29 19:51:00               8.0              8.0          0.02  \n",
       "5719 2017-08-29 19:51:00               8.0              8.0          0.02  \n",
       "5720 2017-08-29 19:51:00               8.0              8.0          0.02  \n",
       "5721 2017-08-29 19:51:00               8.0              8.0          0.02  \n",
       "5722 2017-08-29 19:51:00               8.0              8.0          0.02  \n",
       "5723 2017-08-29 19:51:00               8.0              8.0          0.02  \n",
       "5724 2017-08-29 19:51:00               8.0              8.0          0.02  \n",
       "5725 2017-08-29 19:51:00               8.0              8.0          0.02  \n",
       "5726 2017-08-29 19:51:00               8.0              8.0          0.02  \n",
       "\n",
       "[5727 rows x 11 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()\n",
    "df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecordedAtTime      5727\n",
       "VehicleRef          5727\n",
       "Day                 5727\n",
       "Hour                5727\n",
       "Min                 5727\n",
       "Sec                 5727\n",
       "DATE                5727\n",
       "HOURLYVISIBILITY    5727\n",
       "HOURLYWindSpeed     5727\n",
       "HOURLYPrecip        5727\n",
       "dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(how='all')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          RecordedAtTime VehicleRef  Day  Hour  Min  Sec                DATE  \\\n",
       "5722 2017-08-29 19:32:56  NYCT_8363   29    19   32   56 2017-08-29 19:51:00   \n",
       "5723 2017-08-29 19:32:59  NYCT_8389   29    19   32   59 2017-08-29 19:51:00   \n",
       "5724 2017-08-29 19:42:01  NYCT_4375   29    19   42    1 2017-08-29 19:51:00   \n",
       "5725 2017-08-29 19:42:01  NYCT_4375   29    19   42    1 2017-08-29 19:51:00   \n",
       "5726 2017-08-29 19:42:32  NYCT_8375   29    19   42   32 2017-08-29 19:51:00   \n",
       "\n",
       "      HOURLYVISIBILITY  HOURLYWindSpeed  HOURLYPrecip  \n",
       "5722               8.0              8.0          0.02  \n",
       "5723               8.0              8.0          0.02  \n",
       "5724               8.0              8.0          0.02  \n",
       "5725               8.0              8.0          0.02  \n",
       "5726               8.0              8.0          0.02  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training I\n",
    "Adapted from: https://plot.ly/scikit-learn/plot-gradient-boosting-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving/Loading the Model\n",
    "\n",
    "`pip install joblib`\n",
    "\n",
    "Credit: https://scikit-learn.org/stable/modules/model_persistence.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(gbrt, 'model1.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training II\n",
    "\n",
    "Adapted from: https://shankarmsy.github.io/stories/gbrt-sklearn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing required Python packages \n",
    "%matplotlib inline \n",
    "import matplotlib.pylab as plt \n",
    "import numpy as np \n",
    "from scipy import sparse \n",
    "from sklearn.datasets import make_classification, make_blobs, load_boston, fetch_california_housing \n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.model_selection import (ShuffleSplit, train_test_split, \n",
    "                                     learning_curve, GridSearchCV)\n",
    "from sklearn import metrics \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from pprint import pprint \n",
    "import pandas as pd \n",
    "from pandas.tools.plotting import scatter_matrix \n",
    "import urllib \n",
    "import requests \n",
    "import zipfile \n",
    "import seaborn\n",
    "\n",
    "np.random.seed(sum(map(ord, \"aesthetics\"))) \n",
    "seaborn.set_context('notebook') \n",
    "# pd.set_option('display.mpl_style', 'default') # Make the graphs a bit prettier \n",
    "plt.rcParams['figure.figsize'] = (15, 5) # Set some Pandas options \n",
    "pd.set_option('display.notebook_repr_html', False) \n",
    "pd.set_option('display.max_columns', 40) \n",
    "pd.set_option('display.max_rows', 25) \n",
    "pd.options.display.max_colwidth = 50 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features, Targets and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4581 Min     29.447719\n",
      "Hour    13.416285\n",
      "Sec     28.938441\n",
      "dtype: float64\n",
      "1146 Min     28.204188\n",
      "Hour    13.239965\n",
      "Sec     29.576789\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "features = (['Day', 'Hour', 'Sec', 'Min', 'HOURLYVISIBILITY', 'HOURLYWindSpeed', 'HOURLYPrecip'])\n",
    "\n",
    "target = (['Min', 'Hour', 'Sec'])\n",
    "\n",
    "model_df = df[(features)].dropna().reset_index()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    model_df[features], \n",
    "    model_df[target], test_size=0.2,\n",
    "    random_state=random_state)\n",
    "\n",
    "# train_df[target] = y_train\n",
    "# holdout_df[target] = y_holdout\n",
    "\n",
    "# train_df.reset_index(inplace=True)\n",
    "# holdout_df.reset_index(inplace=True)\n",
    "\n",
    "print(X_train.shape[0], X_train[target].mean())\n",
    "print(X_test.shape[0], X_test[target].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cal=fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split data here\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train_df, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gradient Boosting Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad input shape (4581, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-160-23eba77e2a97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mgbrt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgbrt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mholdout_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m         \u001b[1;31m# Check input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    524\u001b[0m                         dtype=None)\n\u001b[0;32m    525\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'O'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bad input shape {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: bad input shape (4581, 3)"
     ]
    }
   ],
   "source": [
    "gbrt=GradientBoostingRegressor(n_estimators=100) \n",
    "train_df.shape\n",
    "\n",
    "gbrt.fit(X_train, y_train) \n",
    "y_pred=gbrt.predict(holdout_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def GradientBooster(param_grid, n_jobs): \n",
    "        estimator = GradientBoostingRegressor() \n",
    "        #Choose cross-validation generator - let's choose ShuffleSplit which randomly shuffles and selects Train and CV sets \n",
    "        #for each iteration. There are other methods like the KFold split. \n",
    "        cv = ShuffleSplit(X_train.shape[0], test_size=0.2) \n",
    "        \n",
    "        #Apply the cross-validation iterator on the Training set using GridSearchCV. This will run the classifier on the \n",
    "        #different train/cv splits using parameters specified and return the model that has the best results \n",
    "        #Note that we are tuning based on the F1 score 2PR/P+R where P is Precision and R is Recall. This may not always be \n",
    "        #the best score to tune our model on. I will explore this area further in a seperate exercise. For now, we'll use F1. \n",
    "        \n",
    "        classifier = GridSearchCV(estimator=estimator, cv=cv, param_grid=param_grid, n_jobs=n_jobs) \n",
    "        #Also note that we're feeding multiple neighbors to the GridSearch to try out. \n",
    "        #We'll now fit the training dataset to this classifier \n",
    "        classifier.fit(X_train, y_train) \n",
    "        \n",
    "        #Let's look at the best estimator that was found by GridSearchCV \n",
    "        print(\"Best Estimator learned through GridSearch\") \n",
    "        print(classifier.best_estimator_) \n",
    "        return cv, classifier.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Below is a plot_learning_curve module that's provided by scikit-learn. It allows us to quickly and easily visualize how #well the model is performing based on number of samples we're training on. It helps to understand situations such as #high variance or bias. \n",
    "#We'll call this module in the next segment. \n",
    "print(__doc__) \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "# from sklearn import cross_validation \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.datasets import load_digits \n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)): \n",
    "    \"\"\" Generate a simple plot of the test and traning learning curve. \n",
    "    \n",
    "    Parameters \n",
    "    ---------- \n",
    "    estimator : \n",
    "    object type that implements the \"fit\" and \"predict\" methods An object of that type which is cloned for \n",
    "    each validation. title : string Title for the chart. X : array-like, shape (n_samples, n_features) \n",
    "    Training vector, where n_samples is the number of samples and n_features is the number of features. \n",
    "    y : \n",
    "    array-like, shape (n_samples) or (n_samples, n_features), optional Target relative to X for classification \n",
    "    or regression; None for unsupervised learning. ylim : tuple, shape (ymin, ymax), optional Defines minimum \n",
    "    and maximum yvalues plotted. cv : integer, cross-validation generator, optional If an integer is passed, \n",
    "    it is the number of folds (defaults to 3). Specific cross-validation objects can be passed, \n",
    "    see sklearn.cross_validation module for the list of possible objects n_jobs : integer, \n",
    "    optional Number of jobs to run in parallel (default 1). \"\"\" \n",
    "    \n",
    "    plt.figure() \n",
    "    plt.title(title) \n",
    "    if ylim is not None: \n",
    "        plt.ylim(*ylim) \n",
    "    plt.xlabel(\"Training examples\") \n",
    "    plt.ylabel(\"Score\") \n",
    "    train_sizes, train_scores, test_scores = learning_curve( estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes) \n",
    "    train_scores_mean = np.mean(train_scores, axis=1) \n",
    "    train_scores_std = np.std(train_scores, axis=1) \n",
    "    test_scores_mean = np.mean(test_scores, axis=1) \n",
    "    test_scores_std = np.std(test_scores, axis=1) \n",
    "    plt.grid() \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\") \n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\") \n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\") \n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\") \n",
    "    plt.legend(loc=\"best\") \n",
    "    \n",
    "    return plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#WARNING - THIS MIGHT TAKE A WHILE TO RUN. TRY ADJUSTING parameters such as n_jobs (jobs to run in parallel, before \n",
    "#increasing this make sure your system can handle it), n_iter for ShuffleSplit (in the function definition) and reducing \n",
    "#number of values being tried for max_depth/n_estimators. \n",
    "#SELECT INTERRUPT IN THE MENU AND PRESS INTERRUPT KERNEL IF YOU NEEDD TO STOP EXECUTION \n",
    "\n",
    "param_grid={\n",
    "    'n_estimators':[100], \n",
    "    'learning_rate': [0.1],# 0.05, 0.02, 0.01], \n",
    "    'max_depth':[6],#4,6], \n",
    "    'min_samples_leaf':[3],#,5,9,17], \n",
    "    'max_features':[1.0],#,0.3]#,0.1] \n",
    "} \n",
    "\n",
    "n_jobs=4 \n",
    "#Let's fit GBRT to the digits training dataset by calling the function we just created. \n",
    "\n",
    "cv,best_est=GradientBooster(param_grid, n_jobs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OK great, so we got back the best estimator parameters as follows:\n",
    "print(\"Best Estimator Parameters\")\n",
    "print(\"---------------------------\")\n",
    "print(\"n_estimators:\", best_est.n_estimators)\n",
    "print(\"max_depth:\", best_est.max_depth)\n",
    "print(\"Learning Rate:\", best_est.learning_rate)\n",
    "print(\"min_samples_leaf:\", best_est.min_samples_leaf)\n",
    "print(\"max_features:\", best_est.max_features)\n",
    "\n",
    "print(\"Train R-squared:\", best_est.score(X_train,y_train))\n",
    "\n",
    "#Each of these parameters is critical to learning. Some of them will help address overfitting issues as well. For more \n",
    "#info about overfitting and regularization, check out the SVM notebook in my Github repos where I provide more info on \n",
    "#the subject.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The module simply runs the estimator multiple times on subsets of the data provided and plots the train and cv scores.\n",
    "#Note that we're feeding the best parameters we've learned from GridSearchCV to the estimator now.\n",
    "#We may need to adjust the hyperparameters further if there is overfitting (or underfitting, though unlikely)\n",
    "title = \"Learning Curves (Gradient Boosted Regression Trees)\" \n",
    "estimator = GradientBoostingRegressor(n_estimators=best_est.n_estimators, max_depth=best_est.max_depth,\n",
    "                                      learning_rate=best_est.learning_rate, min_samples_leaf=best_est.min_samples_leaf,\n",
    "                                      max_features=best_est.max_features)\n",
    "plot_learning_curve(estimator, title, X_train, y_train, cv=cv, n_jobs=n_jobs)\n",
    "plt.show()\n",
    "\n",
    "#Looks like we've done a reasonable job getting about ~0.85 R-squared on the cv set and looks from the learning\n",
    "#curve that we may be able to do a bit better with more estimators. Although we may need to reduce the learning rate even \n",
    "#further to address any overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's try one more trick. We'll trim the training set to its most important features and re-train to see if \n",
    "#that helps.\n",
    "title = \"Learning Curves (Gradient Boosted Regression Trees)\" \n",
    "\n",
    "#Dropping all parameters except n_estimators and learning_rate since we're going to trim the features anyway.\n",
    "estimator = GradientBoostingRegressor(n_estimators=best_est.n_estimators, learning_rate=best_est.learning_rate)\n",
    "\n",
    "#Calling fit on the estimator so we can transform the X matrices.\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "#Trimming feature matrices to include only those features that are more important than the mean of all importances.\n",
    "# X_train_trim=estimator.transform(X_train, threshold='mean')\n",
    "\n",
    "# #Trimming test as well in case we end up going with this model as final.\n",
    "# X_test_trim=estimator.transform(X_test, threshold='mean')\n",
    "\n",
    "#Re-plotting Learning cruves.\n",
    "plot_learning_curve(estimator, title, X_train, y_train, cv=cv, n_jobs=n_jobs)\n",
    "plt.show()\n",
    "\n",
    "#So what do we infer from this plot? We seem to have addressed overfitting much better but the overall score of both train\n",
    "#and cv has gone down considerably, indicating that the features we dropped were actually collectively contributing\n",
    "#to the model. Let's go back to the first model that the Grid Search returned and run our test scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Switching back to the best model from gridsearch\n",
    "estimator = best_est\n",
    "\n",
    "#Re-fitting to the train set\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "#Calculating train/test scores - R-squared value\n",
    "print(\"Train R-squared: \", estimator.score(X_train, y_train))\n",
    "print(\"Test R-squared: \", estimator.score(X_test, y_test))\n",
    "\n",
    "#There you have it, our final R-squared on the California housing dataset, 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OK let's run through a more complex example this time. We'll explore anonymous loan data provided by lendingclub. \n",
    "#We'll try to predict the interest rate for loan applications based on data provided. Let's first download data to \n",
    "#a pandas df.\n",
    "\n",
    "#The Dataset is a zip file. So let's first read in the dataset through requests then pass it on to Pandas through the\n",
    "#read_csv command\n",
    "url=requests.get('https://resources.lendingclub.com/LoanStats3c.csv.zip')\n",
    "z=zipfile.ZipFile(io.StringIO(url.content))\n",
    "\n",
    "loan=pd.read_csv(z.open('LoanStats3c.csv'), skiprows=1, parse_dates=True, index_col='id')\n",
    "loanbk=loan.copy() #Backup of the dataframe so we don't have to download data everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's take a quick peek at the dataset\n",
    "loan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For simplicity, let's first drop nulls in the dataset. axis=1 indicates we'll drop rows not cols.\n",
    "loan = loan.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OK let's take a look at the columns and see if there are any we can drop any before we get started.\n",
    "loan.columns.values\n",
    "\n",
    "#There're plenty that don't seem very relevant. Let's drop them.\n",
    "loan=loan.drop(['member_id', 'grade', 'sub_grade', 'emp_title', 'issue_d',\n",
    "          'pymnt_plan', 'url', 'desc', 'title', 'initial_list_status',\n",
    "          'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d', 'last_credit_pull_d',\n",
    "          'policy_code', 'emp_length', 'addr_state','zip_code'], axis=1)\n",
    "\n",
    "#Check the data dictionary for this dataset at https://resources.lendingclub.com/LCDataDictionary.xlsx for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get rid of non-numeric values throughout the DataFrame:\n",
    "for col in loan.columns.values:\n",
    "  loan[col] = loan[col].replace('[^0-9]+.-', '', regex=True)\n",
    "loan.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove % symbol from the interest rate & revolving utilization\n",
    "loan.int_rate=loan.int_rate.str.split('%',1).str[0]\n",
    "loan.revol_util=loan.revol_util.str.split('%',1).str[0]\n",
    "\n",
    "#Remove \"months\" from the loan period\n",
    "loan.term=loan.term.str.split(' ',2).str[1]\n",
    "\n",
    "loan.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's change the Income Verified column, which currently has textual labels to numeric.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "loan.is_inc_v = le.fit_transform(loan.is_inc_v.values)\n",
    "loan.home_ownership=le.fit_transform(loan.home_ownership.values)\n",
    "loan.loan_status=le.fit_transform(loan.loan_status.values)\n",
    "loan.purpose=le.fit_transform(loan.purpose.values)\n",
    "\n",
    "#Finally let's be sure we convert all fields to numeric\n",
    "loan=loan.convert_objects(convert_numeric=True)\n",
    "\n",
    "loan.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OK great, let's now get our X and y. We know that interest rate is y.\n",
    "#Pandas is fantastic, all you need to do is use .values to get the data in numpy format\n",
    "y=loan.int_rate.values\n",
    "\n",
    "#Let's remove y from the df so we can get X\n",
    "del loan['int_rate']\n",
    "X=loan.values\n",
    "\n",
    "#Now, the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Great, in no time we have grabbed an unknown dataset from the web, munged it using Pandas and now have ready-to-go\n",
    "#training and test numpy arrays for running the GBRT regressor. Let's go!\n",
    "\n",
    "#WARNING - THIS MIGHT TAKE A WHILE TO RUN. TRY ADJUSTING parameters such as n_jobs (jobs to run in parallel, before \n",
    "#increasing this make sure your system can handle it), n_iter for ShuffleSplit (in the function definition) and reducing \n",
    "#number of values being tried for max_depth/n_estimators.\n",
    "\n",
    "#SELECT INTERRUPT IN THE MENU AND PRESS INTERRUPT KERNEL IF YOU NEEDD TO STOP EXECUTION\n",
    "\n",
    "param_grid={'n_estimators':[100],#,500,1000],\n",
    "            'learning_rate': [0.1,0.05,0.02],# 0.01],\n",
    "            'max_depth':[4,6], \n",
    "            'min_samples_leaf':[3,5,9,17], \n",
    "            'max_features':[1.0,0.3,0.1]\n",
    "           }\n",
    "n_jobs=4\n",
    "\n",
    "#Let's fit GBRT to the digits training dataset by calling the function we just created.\n",
    "cv,best_est=GradientBooster(param_grid, n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OK great, so we got back the best estimator parameters as follows:\n",
    "print(\"Best Estimator Parameters\")\n",
    "print(\"---------------------------\")\n",
    "print (\"n_estimators:\", best_est.n_estimators)\n",
    "print (\"max_depth:\", best_est.max_depth)\n",
    "print (\"Learning Rate:\", best_est.learning_rate)\n",
    "print (\"min_samples_leaf:\", best_est.min_samples_leaf)\n",
    "print (\"max_features:\", best_est.max_features)\n",
    "\n",
    "print (\"Train R-squared:\", best_est.score(X_train,y_train))\n",
    "\n",
    "#The training R-Squared is almost 1.0 which indicates we can understand 99% of the variance in the data as well as\n",
    "#there's a chance we might overfit. Let's see with the learning curves below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OK we'll now call the plot_learning_curve module by feeding it the estimator (best estimator returned from GS) \n",
    "#and train/cv sets.\n",
    "\n",
    "#The module simply runs the estimator multiple times on subsets of the data provided and plots the train and cv scores.\n",
    "#Note that we're feeding the best parameters we've learned from GridSearchCV to the estimator now.\n",
    "#We may need to adjust the hyperparameters further if there is overfitting (or underfitting, though unlikely)\n",
    "title = \"Learning Curves (Gradient Boosted Regression Trees)\" \n",
    "estimator = GradientBoostingRegressor(n_estimators=best_est.n_estimators, max_depth=best_est.max_depth,\n",
    "                                      learning_rate=best_est.learning_rate, min_samples_leaf=best_est.min_samples_leaf,\n",
    "                                      max_features=best_est.max_features)\n",
    "plot_learning_curve(estimator, title, X_train, y_train, cv=cv, n_jobs=n_jobs)\n",
    "plt.show()\n",
    "\n",
    "#OK yes, there is some overfitting there. We can see the training scores in red almost close to 1.0 and the cv scores\n",
    "#trying its best to reach it as the number of examples increases. This is what happens during overfitting. To address\n",
    "#overfitting, GBRT basically has the following parameters we can fine tune: Learning Rate, Max Depth, Min Samples leaf and\n",
    "#Max features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the typical recommended values of Max depth is 4 to 6, so lets leave it at 4. Let's try increasing the min\n",
    "#samples leaf parameter, this basically enforces a lower bound on the number of samples in any given leaf.\n",
    "min_samples_leaf=9\n",
    "\n",
    "title = \"Learning Curves (Gradient Boosted Regression Trees), min_samples_leaf=9\" \n",
    "estimator = GradientBoostingRegressor(n_estimators=best_est.n_estimators, max_depth=best_est.max_depth,\n",
    "                                      learning_rate=best_est.learning_rate, min_samples_leaf=min_samples_leaf,\n",
    "                                      max_features=best_est.max_features)\n",
    "plot_learning_curve(estimator, title, X_train, y_train, cv=cv, n_jobs=n_jobs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's try reducing the max features parameter. This enforces an upper bound of the maximum number of features to use\n",
    "#for training. It's supposed to work well when n_features>30. We'll also remove min samples leaf for this run.\n",
    "max_features=0.5\n",
    "\n",
    "title = \"Learning Curves (Gradient Boosted Regression Trees), max_features=50%\" \n",
    "estimator = GradientBoostingRegressor(n_estimators=best_est.n_estimators, max_depth=best_est.max_depth,\n",
    "                                      learning_rate=best_est.learning_rate, max_features=max_features)\n",
    "plot_learning_curve(estimator, title, X_train, y_train, cv=cv, n_jobs=n_jobs)\n",
    "plt.show()\n",
    "\n",
    "#Nope that didn't quite improve the cv score either. What happens if we reduce learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The lower the learning rate is the more the number of trees we need to train. This is because the rate at which we train\n",
    "#is simply, well, reduced.\n",
    "learning_rate=.01\n",
    "n_estimators=1000\n",
    "\n",
    "title = \"Learning Curves (Gradient Boosted Regression Trees), 1000 Trees at learning rate .01\"\n",
    "estimator = GradientBoostingRegressor(n_estimators=n_estimators, max_depth=best_est.max_depth,\n",
    "                                      learning_rate=learning_rate, min_samples_leaf=best_est.min_samples_leaf,\n",
    "                                      max_features=best_est.max_features)\n",
    "plot_learning_curve(estimator, title, X_train, y_train, cv=cv, n_jobs=n_jobs)\n",
    "plt.show()\n",
    "\n",
    "#Perhaps that improved it a tiny little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Before we try anything else, I would like to explore one of the beautiful advantages of growing trees. And that is to\n",
    "#capture feature importances. Now that we have a publicly available loan application collection (though anonymous), it makes\n",
    "#me really curious to see what impacts the interest rate for a loan application the most.\n",
    "\n",
    "#Let's take a look\n",
    "\n",
    "#Calling fit on the estimator so we can look at feature_importances.\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the feature ranking - Top 10\n",
    "importances = estimator.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print \"Lending Club Loan Data - Top 10 Important Features\\n\"\n",
    "\n",
    "for f in range(10):\n",
    "    print(\"%d. %s   (%f)\" % (f + 1, loan.columns[indices[f]], importances[indices[f]]))\n",
    "    \n",
    "#Plot the feature importances of the forest\n",
    "indices=indices[:10]\n",
    "plt.figure()\n",
    "plt.title(\"Top 10 Feature importances\")\n",
    "plt.bar(range(10), importances[indices],\n",
    "       color=\"r\", align=\"center\")\n",
    "plt.xticks(range(10), loan.columns[indices], fontsize=14, rotation=45)\n",
    "plt.xlim([-1, 10])\n",
    "plt.show()\n",
    "\n",
    "#Mean Feature Importance\n",
    "print \"Mean Feature Importance %.6f\" %np.mean(importances)\n",
    "\n",
    "#Interesting, the total amount of interest received to date is the top most influencer for getting a better interest rate.\n",
    "#Good for the lenders eh? Pay more interest, we'll give you a cut on the interest rate. Of course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Can we actually trim (like before) and get a better result? Perhaps not, but who's to stop us from trying.\n",
    "title = \"Learning Curves (Gradient Boosted Regression Trees) - Trimmed features to > 1% importance\" \n",
    "\n",
    "#Dropping all parameters except n_estimators and learning_rate since we're going to trim the features anyway.\n",
    "estimator = GradientBoostingRegressor(n_estimators=best_est.n_estimators, learning_rate=best_est.learning_rate)\n",
    "\n",
    "#Calling fit on the estimator so we can transform the X matrices.\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "#Trimming feature matrices to include only those features that are more important than the mean of all importances.\n",
    "X_train_trim=estimator.transform(X_train, threshold=.01)\n",
    "\n",
    "#Trimming test as well in case we end up going with this model as final.\n",
    "X_test_trim=estimator.transform(X_test, threshold=.01)\n",
    "\n",
    "#Re-plotting Learning cruves.\n",
    "plot_learning_curve(estimator, title, X_train, y_train, cv=cv, n_jobs=n_jobs)\n",
    "plt.show()\n",
    "\n",
    "#Nope, the curve looks like it overfits less, but look at the cv score, in all our fancy attempts it never really crossed\n",
    "#that ~0.8 R-squared barrier. That tells me, we actually have a decent model at hand and also that a 0.9+ R-squared value is\n",
    "#not always possible, atleast in real time. Let's wrap this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = GradientBoostingRegressor(n_estimators=best_est.n_estimators, max_depth=best_est.max_depth,\n",
    "                                      learning_rate=best_est.learning_rate, min_samples_leaf=best_est.min_samples_leaf,\n",
    "                                      max_features=best_est.max_features)\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "print(\"Final Estimator Parameters\")\n",
    "print(\"---------------------------\")\n",
    "print(\"n_estimators:\", best_est.n_estimators)\n",
    "print(\"max_depth:\", best_est.max_depth)\n",
    "print(\"Learning Rate:\", best_est.learning_rate)\n",
    "print(\"min_samples_leaf:\", best_est.min_samples_leaf)\n",
    "print(\"max_features:\", best_est.max_features)\n",
    "print(\"Final Train R-squared:\", estimator.score(X_train, y_train))\n",
    "print(\"Final Test R-squared:\", estimator.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
